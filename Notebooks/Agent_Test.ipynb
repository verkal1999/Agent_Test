{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaebad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein Zustandsautomat ist ein Modell, das ein System durch eine endliche Anzahl von Zuständen beschreibt und regelt, wie es basierend auf Eingaben von einem Zustand in einen anderen wechselt. Er wird häufig verwendet, um Abläufe und Entscheidungsprozesse in der Informatik oder der Technik darzustellen.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1) Key aus Datei lesen (nur der Key in einer Zeile)\n",
    "key_path = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MA\\OpenAI_API_Key.txt\")\n",
    "api_key = key_path.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "# Optional: simple Plausibilitätsprüfung\n",
    "if not api_key or not any(api_key.startswith(p) for p in (\"sk-\", \"sk-proj-\")):\n",
    "    raise ValueError(\"API-Key in der Datei wirkt ungültig (Präfix fehlt).\")\n",
    "\n",
    "# 2) Client mit Key initialisieren\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# 3) Testaufruf (Responses API, empfohlen)\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    input=\"Erkläre in 2 Sätzen, was ein Zustandsautomat ist – auf Deutsch.\"\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddd7121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objekt-Typen:\n",
      "  GVL: 7\n",
      "  POU: 50\n",
      "\n",
      "Beispiele je Typ:\n",
      "\n",
      "== GVL ==\n",
      "- GVL -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\GVLs\\GVL.TcGVL\n",
      "- GVL_Diagnose -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\GVLs\\GVL_Diagnose.TcGVL\n",
      "- GVL_HRL -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\GVLs\\GVL_HRL.TcGVL\n",
      "- GVL_MBS -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\GVLs\\GVL_MBS.TcGVL\n",
      "- GVL_SST -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\GVLs\\GVL_SST.TcGVL\n",
      "\n",
      "== POU ==\n",
      "- AutomaticColorDetection_nichtfertig  [FunctionBlock/NWL]  IO(in=7, out=3, inout=0) -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\POUs\\AtomicSkillsSchablonen\\AutomaticColorDetection_nichtfertig.TcPOU\n",
      "- AxisControl_MultipleInputSensors  [FunctionBlock/NWL]  IO(in=10, out=6, inout=0) -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\POUs\\AtomicSkillsSchablonen\\AxisControl_MultipleInputSensors.TcPOU\n",
      "- AxisControl_Encoder  [FunctionBlock/NWL]  IO(in=13, out=3, inout=0) -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\POUs\\AtomicSkillsSchablonen\\AxisControl_Encoder.TcPOU\n",
      "- CompressorControl  [FunctionBlock/NWL]  IO(in=23, out=3, inout=0) -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\POUs\\AtomicSkillsSchablonen\\CompressorControl.TcPOU\n",
      "- SecuringWorkpiece  [FunctionBlock/NWL]  IO(in=7, out=3, inout=0) -> C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\SPS_Demonstrator\\POUs\\AtomicSkillsSchablonen\\SecuringWorkpiece.TcPOU\n",
      "\n",
      "Summary: PLCProjs=1, Objects=57, ST-POUs=1\n",
      "\n",
      "Export:\n",
      " - C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\v1.0_Tisch01_Beckhoff_I4.0_objects.json\n",
      " - C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\v1.0_Tisch01_Beckhoff_I4.0_pous_st.json\n",
      "\n",
      "--- ST-POU IO-Details (erste 3) ---\n",
      "\n",
      "POU FB_MyOpcUaMethod (FunctionBlock)\n",
      "  VAR_INPUT:\n",
      "    - in1: INT\n",
      "    - in2: INT\n",
      "  VAR_OUTPUT:\n",
      "    - result: INT\n",
      "  VAR_IN_OUT:\n"
     ]
    }
   ],
   "source": [
    "# TwinCAT: POUs/DUTs/GVLs/VISUs sammeln + ST-IO-Variablen extrahieren (Jupyter-ready)\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import re, json, xml.etree.ElementTree as ET\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def read_text(p: Path) -> str:\n",
    "    return p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "def strip_ns(xml_text: str) -> str:\n",
    "    # Default-Namespaces entfernen -> XPath wird einfacher\n",
    "    return re.sub(r'\\sxmlns=\"[^\"]+\"', '', xml_text, count=1)\n",
    "\n",
    "def strip_st_comments(s: str) -> str:\n",
    "    # ST-Kommentare entfernen: (* ... *) und // ...\n",
    "    s = re.sub(r'\\(\\*.*?\\*\\)', '', s, flags=re.S)\n",
    "    s = re.sub(r'//.*', '', s)\n",
    "    return s\n",
    "\n",
    "def detect_impl_lang(impl_node):\n",
    "    \"\"\"Finde ST/FBD/LD/SFC/IL auch wenn ein NWL-Container dazwischen sitzt.\"\"\"\n",
    "    if impl_node is None:\n",
    "        return None, \"\"\n",
    "    for tag in (\"ST\", \"FBD\", \"LD\", \"SFC\", \"IL\"):\n",
    "        n = impl_node.find(f\".//{tag}\")\n",
    "        if n is not None:\n",
    "            return tag, (n.text or \"\").strip()\n",
    "    # Fallback: erster Child-Tagname (z. B. 'NWL')\n",
    "    if list(impl_node):\n",
    "        c = list(impl_node)[0]\n",
    "        return c.tag, (c.text or \"\").strip()\n",
    "    return None, \"\"\n",
    "\n",
    "# ---------- IEC/ST Deklarationsparser ----------\n",
    "_var_stmt_re = re.compile(\n",
    "    r'^\\s*([A-Za-z_]\\w*)'               # Name\n",
    "    r'(?:\\s+AT\\s+([^:]+))?'             # optional AT-Adresse\n",
    "    r'\\s*:\\s*'                          \n",
    "    r'([^:=;]+?)'                       # Typ (inkl. ARRAY[..] OF ...)\n",
    "    r'(?:\\s*:=\\s*([^;]+?))?'            # optional Initialwert\n",
    "    r'\\s*;\\s*$', re.M | re.S)\n",
    "\n",
    "def _extract_var_block(text: str, scope_keyword: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extrahiert Variablen aus einem Block VAR_<SCOPE> ... END_VAR.\n",
    "    scope_keyword: 'INPUT' | 'OUTPUT' | 'IN_OUT' | 'GLOBAL' | 'TEMP' | etc.\n",
    "    \"\"\"\n",
    "    txt = strip_st_comments(text)\n",
    "    # Nicht-gierige Suche inkl. evtl. Zusätzen wie CONSTANT/RETAIN nach VAR_<SCOPE>\n",
    "    m = re.search(rf'VAR_{scope_keyword}\\b.*?\\n(.*?)END_VAR', txt, flags=re.S | re.I)\n",
    "    if not m:\n",
    "        return []\n",
    "    block = m.group(1)\n",
    "    vars_ = []\n",
    "    # Auf Semikolons getrimmt parsen\n",
    "    for m2 in _var_stmt_re.finditer(block):\n",
    "        name, at_addr, typ, init = [g.strip() if g else None for g in m2.groups()]\n",
    "        vars_.append({\n",
    "            \"name\": name,\n",
    "            \"address\": at_addr,\n",
    "            \"type\": re.sub(r'\\s+', ' ', typ).strip(),\n",
    "            \"init\": init.strip() if init else None\n",
    "        })\n",
    "    return vars_\n",
    "\n",
    "def extract_io_from_declaration(declaration: str) -> dict:\n",
    "    \"\"\"Liest IO-Variablen aus der ST-Deklaration.\"\"\"\n",
    "    return {\n",
    "        \"inputs\": _extract_var_block(declaration, \"INPUT\"),\n",
    "        \"outputs\": _extract_var_block(declaration, \"OUTPUT\"),\n",
    "        \"inouts\": _extract_var_block(declaration, \"IN_OUT\"),\n",
    "        # Optional: lokale Blöcke, falls gewünscht\n",
    "        \"temps\": _extract_var_block(declaration, \"TEMP\"),\n",
    "    }\n",
    "\n",
    "# ---------- Parser für TwinCAT-XML ----------\n",
    "def parse_tc_pou_anylang(pou_path: Path):\n",
    "    txt = read_text(pou_path)\n",
    "    root = ET.fromstring(strip_ns(txt))\n",
    "\n",
    "    pou = root.find(\".//POU\")\n",
    "    name = pou.get(\"Name\") if pou is not None else pou_path.stem\n",
    "\n",
    "    # Typ (Program / FunctionBlock / Function)\n",
    "    ptype = (pou.get(\"POUType\") if pou is not None else \"\") or \"\"\n",
    "    decl_node = root.find(\".//Declaration\")\n",
    "    declaration = (decl_node.text or \"\").strip() if decl_node is not None else \"\"\n",
    "    if not ptype and declaration:\n",
    "        m = re.match(r\"\\s*(PROGRAM|FUNCTION_BLOCK|FUNCTION)\\b\", declaration, re.I)\n",
    "        ptype = (m.group(1).title().replace(\"_\", \"\") if m else \"\")\n",
    "\n",
    "    impl_node = root.find(\".//Implementation\")\n",
    "    lang_tag, impl_text = detect_impl_lang(impl_node)\n",
    "\n",
    "    io = extract_io_from_declaration(declaration) if declaration else {\"inputs\":[], \"outputs\":[], \"inouts\":[], \"temps\":[]}\n",
    "\n",
    "    return {\n",
    "        \"kind\": \"POU\",\n",
    "        \"name\": name,\n",
    "        \"pou_type\": ptype,                  # Program | FunctionBlock | Function\n",
    "        \"implementation_lang\": lang_tag,    # ST | FBD | LD | SFC | IL | NWL | None\n",
    "        \"declaration\": declaration,\n",
    "        \"implementation\": impl_text,        # bei FBD/LD meist leer (grafisch)\n",
    "        \"io\": io,\n",
    "        \"file\": str(pou_path)\n",
    "    }\n",
    "\n",
    "def parse_tc_dut(dut_path: Path):\n",
    "    txt = read_text(dut_path)\n",
    "    root = ET.fromstring(strip_ns(txt))\n",
    "    dut = root.find(\".//DUT\")\n",
    "    name = dut.get(\"Name\") if dut is not None else dut_path.stem\n",
    "    # Typ (STRUCT/ENUM/ALIAS/UNION) steckt i. d. R. in der Declaration\n",
    "    decl_node = root.find(\".//Declaration\")\n",
    "    declaration = (decl_node.text or \"\").strip() if decl_node is not None else \"\"\n",
    "    # heuristischer dut_kind\n",
    "    dut_kind = \"\"\n",
    "    m = re.match(r\"\\s*(TYPE\\s+)?(STRUCT|ENUM|UNION|ALIAS)\\b\", declaration, re.I)\n",
    "    if m:\n",
    "        dut_kind = m.group(2).upper()\n",
    "    return {\n",
    "        \"kind\": \"DUT\",\n",
    "        \"name\": name,\n",
    "        \"dut_kind\": dut_kind,\n",
    "        \"declaration\": declaration,\n",
    "        \"file\": str(dut_path)\n",
    "    }\n",
    "\n",
    "def parse_tc_gvl(gvl_path: Path):\n",
    "    txt = read_text(gvl_path)\n",
    "    root = ET.fromstring(strip_ns(txt))\n",
    "    gvl = root.find(\".//GVL\")\n",
    "    name = gvl.get(\"Name\") if gvl is not None else gvl_path.stem\n",
    "    decl_node = root.find(\".//Declaration\")\n",
    "    declaration = (decl_node.text or \"\").strip() if decl_node is not None else \"\"\n",
    "    # Variablen in GVL stehen üblicherweise in VAR_GLOBAL ... END_VAR\n",
    "    globals_ = _extract_var_block(declaration, \"GLOBAL\")\n",
    "    return {\n",
    "        \"kind\": \"GVL\",\n",
    "        \"name\": name,\n",
    "        \"declaration\": declaration,\n",
    "        \"globals\": globals_,\n",
    "        \"file\": str(gvl_path)\n",
    "    }\n",
    "\n",
    "def parse_tc_vis(vis_path: Path):\n",
    "    \"\"\"\n",
    "    VISU-Metadaten aus .TcVis (Seitenname). Struktur ist XML; wir lesen den Wurzelknoten.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        txt = read_text(vis_path)\n",
    "        root = ET.fromstring(strip_ns(txt))\n",
    "        vis = root.find(\".//Visualization\")\n",
    "        name = (vis.get(\"Name\") if vis is not None else None) or vis_path.stem\n",
    "    except Exception:\n",
    "        name = vis_path.stem\n",
    "    return {\n",
    "        \"kind\": \"VISU\",\n",
    "        \"name\": name,\n",
    "        \"file\": str(vis_path)\n",
    "    }\n",
    "\n",
    "# ---------- .plcproj Utilities ----------\n",
    "def list_artifacts_in_plcproj(plcproj: Path):\n",
    "    \"\"\"\n",
    "    Sucht referenzierte .TcPOU/.TcDUT/.TcGVL/.TcVis und zusätzlich inline-Objekte im .plcproj.\n",
    "    \"\"\"\n",
    "    txt = strip_ns(read_text(plcproj))\n",
    "    root = ET.fromstring(txt)\n",
    "    out = []\n",
    "\n",
    "    # 1) Referenzen in ItemGroups\n",
    "    for item in root.findall(\".//ItemGroup/*\"):\n",
    "        inc = item.get(\"Include\") or \"\"\n",
    "        inc_l = inc.lower()\n",
    "        p = (plcproj.parent / inc).resolve()\n",
    "\n",
    "        try:\n",
    "            if inc_l.endswith(\".tcpou\") and p.exists():\n",
    "                out.append(parse_tc_pou_anylang(p))\n",
    "            elif inc_l.endswith(\".tcdut\") and p.exists():\n",
    "                out.append(parse_tc_dut(p))\n",
    "            elif inc_l.endswith(\".tcgvl\") and p.exists():\n",
    "                out.append(parse_tc_gvl(p))\n",
    "            elif inc_l.endswith(\".tcvis\") and p.exists():\n",
    "                out.append(parse_tc_vis(p))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Fehler beim Parsen {p}: {e}\")\n",
    "\n",
    "    # 2) Inline-POUs/GVLs/DUTs (falls Multiple Project Files nicht aktiv war)\n",
    "    for pou in root.findall(\".//POU\"):\n",
    "        name = pou.get(\"Name\") or \"\"\n",
    "        ptype = pou.get(\"POUType\") or \"\"\n",
    "        decl = pou.find(\".//Declaration\")\n",
    "        impl = pou.find(\".//Implementation\")\n",
    "        lang_tag, impl_text = detect_impl_lang(impl)\n",
    "        declaration = (decl.text or \"\").strip() if decl is not None else \"\"\n",
    "        out.append({\n",
    "            \"kind\": \"POU\",\n",
    "            \"name\": name,\n",
    "            \"pou_type\": ptype,\n",
    "            \"implementation_lang\": lang_tag,\n",
    "            \"declaration\": declaration,\n",
    "            \"implementation\": impl_text,\n",
    "            \"io\": extract_io_from_declaration(declaration) if declaration else {\"inputs\":[], \"outputs\":[], \"inouts\":[], \"temps\":[]},\n",
    "            \"file\": str(plcproj) + \" (inline)\"\n",
    "        })\n",
    "    for gvl in root.findall(\".//GVL\"):\n",
    "        name = gvl.get(\"Name\") or \"\"\n",
    "        decl = gvl.find(\".//Declaration\")\n",
    "        declaration = (decl.text or \"\").strip() if decl is not None else \"\"\n",
    "        out.append({\n",
    "            \"kind\": \"GVL\",\n",
    "            \"name\": name,\n",
    "            \"declaration\": declaration,\n",
    "            \"globals\": _extract_var_block(declaration, \"GLOBAL\"),\n",
    "            \"file\": str(plcproj) + \" (inline)\"\n",
    "        })\n",
    "    for dut in root.findall(\".//DUT\"):\n",
    "        name = dut.get(\"Name\") or \"\"\n",
    "        decl = dut.find(\".//Declaration\")\n",
    "        declaration = (decl.text or \"\").strip() if decl is not None else \"\"\n",
    "        m = re.match(r\"\\s*(TYPE\\s+)?(STRUCT|ENUM|UNION|ALIAS)\\b\", declaration, re.I)\n",
    "        dut_kind = m.group(2).upper() if m else \"\"\n",
    "        out.append({\n",
    "            \"kind\": \"DUT\",\n",
    "            \"name\": name,\n",
    "            \"dut_kind\": dut_kind,\n",
    "            \"declaration\": declaration,\n",
    "            \"file\": str(plcproj) + \" (inline)\"\n",
    "        })\n",
    "    # VISUs sind selten inline; falls vorhanden:\n",
    "    for vis in root.findall(\".//Visualization\"):\n",
    "        name = vis.get(\"Name\") or \"\"\n",
    "        out.append({\n",
    "            \"kind\": \"VISU\",\n",
    "            \"name\": name,\n",
    "            \"file\": str(plcproj) + \" (inline)\"\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "def find_tsprojs_in_sln(sln_path: Path):\n",
    "    txt = read_text(sln_path)\n",
    "    tsprojs = []\n",
    "    for m in re.finditer(r'Project\\(\".*?\"\\)\\s=\\s*\".*?\",\\s*\"(.*?)\"', txt):\n",
    "        rel = m.group(1)\n",
    "        if rel.lower().endswith(\".tsproj\"):\n",
    "            tsprojs.append((sln_path.parent / rel).resolve())\n",
    "    return tsprojs\n",
    "\n",
    "def find_plcprojs_near(tsproj: Path):\n",
    "    return list(tsproj.parent.rglob(\"*.plcproj\"))\n",
    "\n",
    "# ---------- Pfad zu DEINER SLN ----------\n",
    "sln_path = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\v1.0_Tisch01_Beckhoff_I4.0.sln\")\n",
    "\n",
    "# ---------- Sammeln ----------\n",
    "tsprojs = find_tsprojs_in_sln(sln_path)\n",
    "plcprojs = []\n",
    "for ts in tsprojs:\n",
    "    plcprojs.extend(find_plcprojs_near(ts))\n",
    "plcprojs = sorted(set(plcprojs))\n",
    "\n",
    "all_objs = []\n",
    "for pp in plcprojs:\n",
    "    try:\n",
    "        all_objs.extend(list_artifacts_in_plcproj(pp))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Fehler beim Parsen {pp}: {e}\")\n",
    "\n",
    "# ---------- Auswertung ----------\n",
    "kinds = Counter([o.get(\"kind\") for o in all_objs])\n",
    "print(\"Objekt-Typen:\")\n",
    "for k, v in kinds.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Beispiele je Typ\n",
    "by_kind = defaultdict(list)\n",
    "for o in all_objs:\n",
    "    by_kind[o.get(\"kind\")].append(o)\n",
    "\n",
    "print(\"\\nBeispiele je Typ:\")\n",
    "for kind, items in by_kind.items():\n",
    "    print(f\"\\n== {kind} ==\")\n",
    "    for o in items[:5]:  # max 5 Beispiele\n",
    "        if kind == \"POU\":\n",
    "            io = o.get(\"io\", {})\n",
    "            io_sum = f\"in={len(io.get('inputs',[]))}, out={len(io.get('outputs',[]))}, inout={len(io.get('inouts',[]))}\"\n",
    "            print(f\"- {o['name']}  [{o.get('pou_type','?')}/{o.get('implementation_lang') or '—'}]  IO({io_sum}) -> {o['file']}\")\n",
    "        elif kind == \"DUT\":\n",
    "            print(f\"- {o['name']}  [{o.get('dut_kind') or '—'}] -> {o['file']}\")\n",
    "        else:\n",
    "            print(f\"- {o['name']} -> {o['file']}\")\n",
    "\n",
    "# Nur POUs mit ST-Implementation zeigen + deren IO-Variablen\n",
    "st_pous = [o for o in all_objs if o.get(\"kind\")==\"POU\" and (o.get(\"implementation_lang\") or \"\").upper()==\"ST\"]\n",
    "print(f\"\\nSummary: PLCProjs={len(plcprojs)}, Objects={len(all_objs)}, ST-POUs={len(st_pous)}\")\n",
    "\n",
    "# Optional: Dateien schreiben (JSONs neben der SLN)\n",
    "out_base = sln_path.with_suffix(\"\")\n",
    "Path(str(out_base) + \"_objects.json\").write_text(json.dumps(all_objs, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "Path(str(out_base) + \"_pous_st.json\").write_text(json.dumps(st_pous, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(\"\\nExport:\")\n",
    "print(\" -\", str(out_base) + \"_objects.json\")\n",
    "print(\" -\", str(out_base) + \"_pous_st.json\")\n",
    "\n",
    "# Beispielhafte Ausgabe der IO-Listen und ST-Implementierung (gekürzt) für die ersten 3 ST-POUs\n",
    "print(\"\\n--- ST-POU IO-Details (erste 3) ---\")\n",
    "for o in st_pous[:3]:\n",
    "    print(f\"\\nPOU {o['name']} ({o.get('pou_type','?')})\")\n",
    "    io = o[\"io\"]\n",
    "    for label, lst in [(\"VAR_INPUT\", io[\"inputs\"]), (\"VAR_OUTPUT\", io[\"outputs\"]), (\"VAR_IN_OUT\", io[\"inouts\"])]:\n",
    "        print(f\"  {label}:\")\n",
    "        for v in lst:\n",
    "            addr = f\" @ {v['address']}\" if v['address'] else \"\"\n",
    "            init = f\" := {v['init']}\" if v['init'] else \"\"\n",
    "            print(f\"    - {v['name']}: {v['type']}{addr}{init}\")\n",
    "    # ST-Code (falls vorhanden) leicht gekürzt\n",
    "    impl = (o.get(\"implementation\") or \"\").strip()\n",
    "    if impl:\n",
    "        preview = impl if len(impl) < 800 else impl[:800] + \"\\n... [gekürzt] ...\"\n",
    "        print(\"\\n  ST-Implementation (Preview):\\n\" + preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efbfdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene PLC-Projekte: ['TIPC^SPS_Demonstrator^SPS_Demonstrator Project']\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib\n",
    "\n",
    "# 1) JSON einlesen\n",
    "json_path = pathlib.Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\v1.0_Tisch01_Beckhoff_I4.0_objects.json\")\n",
    "with json_path.open(encoding=\"utf-8\") as f:\n",
    "    objects = json.load(f)\n",
    "\n",
    "plc_names = set()\n",
    "\n",
    "# 2) Für jedes Artefakt den Elternordner durchsuchen, bis .plcproj gefunden wird\n",
    "for obj in objects:\n",
    "    fpath = pathlib.Path(obj[\"file\"])\n",
    "    # inline-Einträge haben \" (inline)\" am Ende, deshalb originalen Pfad extrahieren\n",
    "    try:\n",
    "        fpath = pathlib.Path(fpath.as_posix().split(\" (inline)\")[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    for parent in fpath.parents:\n",
    "        for plcproj in parent.glob(\"*.plcproj\"):\n",
    "            plc_names.add(plcproj.stem)\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "\n",
    "# 3) Aus PLC-Namen Lookup-Pfade bauen\n",
    "lookup_paths = [f\"TIPC^{name}^{name} Project\" for name in sorted(plc_names)]\n",
    "print(\"Gefundene PLC-Projekte:\", lookup_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987dbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pywin32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc5925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLC-Namen aus Dateien (nur Info): ['SPS_Demonstrator']\n",
      "Projects in Solution:\n",
      "  Index 1: Name=Beckhoff_I4.0-Demonstrator, FullName=C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\Test1\\Beckhoff_I4.0-Demonstrator.tsproj\n",
      "Verwende TwinCAT-Projekt: Beckhoff_I4.0-Demonstrator\n",
      "PLC-Root gefunden: SPS Pfad: TIPC\n",
      "  Kind: SPS_Demonstrator | Pfad: TIPC^SPS_Demonstrator\n",
      "Versuche Export via NestedProject von 'SPS_Demonstrator' ...\n",
      "XML-Export erstellt: C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\export.xml\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, datetime, pythoncom\n",
    "import win32com.client as com\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Konfiguration ---\n",
    "sln_path   = r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\v1.0_Tisch01_Beckhoff_I4.0.sln\"\n",
    "export_xml = r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\export.xml\"\n",
    "json_path  = r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\v1.0_Tisch01_Beckhoff_I4.0_objects.json\"\n",
    "\n",
    "# --- PLC-Namen (nur Info / Fallback bei Pfadkonstruktion) ---\n",
    "with open(json_path, encoding=\"utf-8\") as f:\n",
    "    objects = json.load(f)\n",
    "plc_names = set()\n",
    "for obj in objects:\n",
    "    fpath = Path(obj[\"file\"].split(\" (inline)\")[0])\n",
    "    for parent in fpath.parents:\n",
    "        for plcproj in parent.glob(\"*.plcproj\"):\n",
    "            plc_names.add(plcproj.stem)\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "print(\"PLC-Namen aus Dateien (nur Info):\", sorted(plc_names))\n",
    "\n",
    "# --- TwinCAT Solution öffnen ---\n",
    "Path(export_xml).parent.mkdir(parents=True, exist_ok=True)\n",
    "dte = com.Dispatch(\"TcXaeShell.DTE.17.0\")  # ggf. Version anpassen\n",
    "dte.SuppressUI = False\n",
    "dte.MainWindow.Visible = True\n",
    "solution = dte.Solution\n",
    "solution.Open(sln_path)\n",
    "\n",
    "print(\"Projects in Solution:\")\n",
    "for i in range(1, solution.Projects.Count + 1):\n",
    "    p = solution.Projects.Item(i)\n",
    "    print(f\"  Index {i}: Name={p.Name}, FullName={p.FullName}\")\n",
    "\n",
    "# .tsproj ermitteln\n",
    "tc_project = None\n",
    "for i in range(1, solution.Projects.Count + 1):\n",
    "    p = solution.Projects.Item(i)\n",
    "    if p.FullName.lower().endswith(\".tsproj\"):\n",
    "        tc_project = p\n",
    "        break\n",
    "if tc_project is None:\n",
    "    raise RuntimeError(\"Kein TwinCAT-Systemprojekt (.tsproj) in der Solution gefunden\")\n",
    "print(\"Verwende TwinCAT-Projekt:\", tc_project.Name)\n",
    "\n",
    "sys_mgr = tc_project.Object  # ITcSysManager\n",
    "\n",
    "# --- PLC-Root & Kinder ermitteln ---\n",
    "root_plc = sys_mgr.LookupTreeItem(\"TIPC\")\n",
    "print(\"PLC-Root gefunden:\", root_plc.Name, \"Pfad:\", root_plc.PathName)\n",
    "\n",
    "children = []\n",
    "try:\n",
    "    # Bevorzugt Enumerator (COM _NewEnum)\n",
    "    for child in root_plc:\n",
    "        print(\"  Kind:\", child.Name, \"| Pfad:\", child.PathName)\n",
    "        children.append(child)\n",
    "except Exception as e:\n",
    "    # Fallback 1-basiert\n",
    "    print(\"Enumerator nicht verfügbar -> Child(i). Grund:\", e)\n",
    "    cnt = int(root_plc.ChildCount)\n",
    "    for i in range(1, cnt + 1):\n",
    "        child = root_plc.Child(i)\n",
    "        print(\"  Kind:\", child.Name, \"| Pfad:\", child.PathName)\n",
    "        children.append(child)\n",
    "\n",
    "if not children:\n",
    "    raise RuntimeError(\"Unter 'TIPC' wurde kein PLC-Projekt gefunden.\")\n",
    "\n",
    "# --- Export-Funktion ---\n",
    "def try_export_from_node(node, out_path: Path, selection: str = \"\"):\n",
    "    \"\"\"Versucht, PlcOpenExport auf einem Knoten mit ITcPlcIECProject aufzurufen.\"\"\"\n",
    "    # Datei freimachen oder alternativen Namen wählen\n",
    "    target = out_path\n",
    "    if target.exists():\n",
    "        try:\n",
    "            target.unlink()\n",
    "        except Exception:\n",
    "            ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            target = target.with_name(f\"{target.stem}_{ts}{target.suffix}\")\n",
    "            print(\"Konnte bestehende Datei nicht löschen -> nutze:\", target)\n",
    "\n",
    "    node.PlcOpenExport(str(target), selection)\n",
    "    print(\"XML-Export erstellt:\", target)\n",
    "    return target\n",
    "\n",
    "exported = False\n",
    "last_err = None\n",
    "\n",
    "# 1) Primär: NestedProject benutzen (robust gegen Bezeichner/Übersetzung)\n",
    "for child in children:\n",
    "    print(f\"Versuche Export via NestedProject von '{child.Name}' ...\")\n",
    "    try:\n",
    "        nested = child.NestedProject  # ITcPlcIECProject\n",
    "        try_export_from_node(nested, Path(export_xml), selection=\"\")  # leer = gesamtes NestedProject\n",
    "        exported = True\n",
    "        break\n",
    "    except pythoncom.com_error as e:\n",
    "        print(\"  NestedProject/Export nicht möglich bei\", child.Name, \"->\", e)\n",
    "        last_err = e\n",
    "\n",
    "# 2) Sekundär: Explizite Pfade testen — sowohl '... Project' (EN) als auch '... Projekt' (DE)\n",
    "if not exported:\n",
    "    candidates = []\n",
    "    for child in children:\n",
    "        base = child.PathName           # z. B. TIPC^SPS_Demonstrator\n",
    "        name = child.Name               # z. B. SPS_Demonstrator\n",
    "        # Reihenfolge: erst 'Project', dann 'Projekt', dann nackter Name (manche Bäume haben kein Suffix)\n",
    "        candidates += [\n",
    "            f\"{base}^{name} Project\",\n",
    "            f\"{base}^{name} Projekt\",\n",
    "            f\"{base}^{name}\",\n",
    "        ]\n",
    "    # auch aus JSON bekannte Namen stützen\n",
    "    for nm in sorted(plc_names):\n",
    "        candidates += [f\"TIPC^{nm}^{nm} Project\", f\"TIPC^{nm}^{nm} Projekt\", f\"TIPC^{nm}\"]\n",
    "\n",
    "    # Deduplizieren, Reihenfolge beibehalten\n",
    "    seen = set(); uniq = []\n",
    "    for c in candidates:\n",
    "        if c not in seen:\n",
    "            uniq.append(c); seen.add(c)\n",
    "\n",
    "    print(\"Probiere Pfad-Kandidaten:\")\n",
    "    for c in uniq:\n",
    "        try:\n",
    "            node = sys_mgr.LookupTreeItem(c)\n",
    "            print(\"  [OK] gefunden:\", c)\n",
    "            try:\n",
    "                try_export_from_node(node, Path(export_xml), selection=\"\")\n",
    "                exported = True\n",
    "                break\n",
    "            except pythoncom.com_error as e:\n",
    "                print(\"    -> Knoten gefunden, aber Export schlug fehl:\", e)\n",
    "                last_err = e\n",
    "        except pythoncom.com_error as e:\n",
    "            print(\"  [--] nicht gefunden:\", c, \"| Grund:\", e)\n",
    "            last_err = e\n",
    "\n",
    "if not exported:\n",
    "    raise RuntimeError(f\"Kein exportierbarer PLC-Knoten gefunden. Letzter Fehler: {last_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f929e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "NS = {'ns': 'http://www.plcopen.org/xml/tc6_0200'}\n",
    "\n",
    "def parse_io_vars(pou):\n",
    "    \"\"\"Liefert Listen der deklarierten Inputs und Outputs aus der Interface-Sektion eines POU.\"\"\"\n",
    "    inputs, outputs = [], []\n",
    "    interface = pou.find('ns:interface', NS)\n",
    "    if interface is not None:\n",
    "        input_vars = interface.find('ns:inputVars', NS)\n",
    "        if input_vars is not None:\n",
    "            for var in input_vars.findall('ns:variable', NS):\n",
    "                name = var.attrib.get('name')\n",
    "                if name:\n",
    "                    inputs.append(name)\n",
    "        output_vars = interface.find('ns:outputVars', NS)\n",
    "        if output_vars is not None:\n",
    "            for var in output_vars.findall('ns:variable', NS):\n",
    "                name = var.attrib.get('name')\n",
    "                if name:\n",
    "                    outputs.append(name)\n",
    "    return inputs, outputs\n",
    "\n",
    "def build_node_mapping(fbd):\n",
    "    \"\"\"Erzeugt ein Dictionary localId -> externer Ausdruck für inVariable/outVariable-Knoten.\"\"\"\n",
    "    node_expr = {}\n",
    "    for inv in fbd.findall('ns:inVariable', NS):\n",
    "        lid = inv.get('localId')\n",
    "        expr = inv.find('ns:expression', NS)\n",
    "        if lid and expr is not None and expr.text:\n",
    "            node_expr[lid] = expr.text.strip()\n",
    "    for outv in fbd.findall('ns:outVariable', NS):\n",
    "        lid = outv.get('localId')\n",
    "        expr = outv.find('ns:expression', NS)\n",
    "        if lid and expr is not None and expr.text:\n",
    "            node_expr[lid] = expr.text.strip()\n",
    "    return node_expr\n",
    "\n",
    "def extract_call_blocks(fbd, pou_names_set, node_map):\n",
    "    \"\"\"Sammelt die Aufrufe von Unterprogrammen (block.typeName in pou_names_set) und deren I/O-Mapping.\"\"\"\n",
    "    calls = []\n",
    "    for block in fbd.findall('ns:block', NS):\n",
    "        type_name = block.get('typeName')\n",
    "        if type_name and type_name in pou_names_set:\n",
    "            call_info = {\n",
    "                'SubNetwork_Name': type_name,\n",
    "                'instanceName': block.get('instanceName'),\n",
    "                'inputs': [],\n",
    "                'outputs': [],\n",
    "            }\n",
    "            # Eingänge der Subfunktion auslesen\n",
    "            for var in block.findall('ns:inputVariables/ns:variable', NS):\n",
    "                formal = var.get('formalParameter')\n",
    "                ext = None\n",
    "                cpin = var.find('ns:connectionPointIn', NS)\n",
    "                if cpin is not None:\n",
    "                    conn = cpin.find('ns:connection', NS)\n",
    "                    if conn is not None:\n",
    "                        ref = conn.get('refLocalId')\n",
    "                        if ref:\n",
    "                            ext = node_map.get(ref, f'localId:{ref}')\n",
    "                call_info['inputs'].append({'internal': formal, 'external': ext})\n",
    "            # Ausgänge der Subfunktion auslesen\n",
    "            for var in block.findall('ns:outputVariables/ns:variable', NS):\n",
    "                formal = var.get('formalParameter')\n",
    "                ext = None\n",
    "                cpout = var.find('ns:connectionPointOut', NS)\n",
    "                if cpout is not None:\n",
    "                    expr = cpout.find('ns:expression', NS)\n",
    "                    if expr is not None and expr.text:\n",
    "                        ext = expr.text.strip()\n",
    "                    else:\n",
    "                        conn = cpout.find('ns:connection', NS)\n",
    "                        if conn is not None:\n",
    "                            ref = conn.get('refLocalId')\n",
    "                            if ref:\n",
    "                                ext = node_map.get(ref, f'localId:{ref}')\n",
    "                call_info['outputs'].append({'internal': formal, 'external': ext})\n",
    "            calls.append(call_info)\n",
    "    return calls\n",
    "\n",
    "def map_pou_io_to_external(pou, node_map):\n",
    "    \"\"\"\n",
    "    Ordnet deklarierten Inputs/Outputs eines POU den externen Variablennamen zu,\n",
    "    sofern sie in den in/out-Variablen des FBD-Blocks erscheinen.\n",
    "    \"\"\"\n",
    "    inputs, outputs = parse_io_vars(pou)\n",
    "    mapped_inputs = []\n",
    "    mapped_outputs = []\n",
    "    # Reverse-Mapping: Wenn das Ausdrucks-Suffix dem internen Namen entspricht, wird es als externe Variable verwendet.\n",
    "    for inp in inputs:\n",
    "        ext = None\n",
    "        for expr in node_map.values():\n",
    "            if expr.split('.')[-1] == inp:\n",
    "                ext = expr\n",
    "                break\n",
    "        mapped_inputs.append({'internal': inp, 'external': ext})\n",
    "    for out in outputs:\n",
    "        ext = None\n",
    "        for expr in node_map.values():\n",
    "            if expr.split('.')[-1] == out:\n",
    "                ext = expr\n",
    "                break\n",
    "        mapped_outputs.append({'internal': out, 'external': ext})\n",
    "    return mapped_inputs, mapped_outputs\n",
    "\n",
    "def analyze_plcopen(xml_path):\n",
    "    \"\"\"Analysiert die PLCopen-XML und erzeugt eine Liste aus Programminformationen und Subnetz-Aufrufen.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    pou_names = {p.attrib.get('name') for p in root.findall('.//ns:pou', NS)}\n",
    "    result = []\n",
    "    for pou in root.findall('.//ns:pou', NS):\n",
    "        name = pou.attrib.get('name')\n",
    "        fbd = pou.find('.//ns:FBD', NS)\n",
    "        node_map = build_node_mapping(fbd) if fbd is not None else {}\n",
    "        inputs, outputs = parse_io_vars(pou)\n",
    "        mapped_inputs, mapped_outputs = ([], [])\n",
    "        if fbd is not None:\n",
    "            mapped_inputs, mapped_outputs = map_pou_io_to_external(pou, node_map)\n",
    "        else:\n",
    "            mapped_inputs = [{'internal': n, 'external': None} for n in inputs]\n",
    "            mapped_outputs = [{'internal': n, 'external': None} for n in outputs]\n",
    "        subcalls = extract_call_blocks(fbd, pou_names, node_map) if fbd is not None else []\n",
    "        result.append({\n",
    "            'Programm_Name': name,\n",
    "            'inputs': mapped_inputs,\n",
    "            'outputs': mapped_outputs,\n",
    "            'subcalls': subcalls\n",
    "        })\n",
    "    return result\n",
    "\n",
    "# Beispielaufruf:\n",
    "xml_file = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\export.xml\")\n",
    "mapping = analyze_plcopen(xml_file)\n",
    "with open(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\program_io_with_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62cd52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON um interne Variablen und Programmkode erweitert.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# 1) JSON erneut laden\n",
    "json_file = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\program_io_with_mapping.json\")\n",
    "mapping = json.loads(json_file.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# 2) export.xml parsen\n",
    "xml_file = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\export.xml\")\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "NS = {\"ns\": \"http://www.plcopen.org/xml/tc6_0200\",\n",
    "      \"html\": \"http://www.w3.org/1999/xhtml\"}\n",
    "\n",
    "# 3) interne Variablen und Programmkode je POU sammeln\n",
    "pou_info = {}\n",
    "for pou in root.findall(\".//ns:pou\", NS):\n",
    "    name = pou.attrib.get(\"name\")\n",
    "    # interne Variablen\n",
    "    locals_list = []\n",
    "    local_vars = pou.find('ns:interface/ns:localVars', NS)\n",
    "    if local_vars is not None:\n",
    "        for v in local_vars.findall('ns:variable', NS):\n",
    "            vname = v.attrib.get('name')\n",
    "            if vname:\n",
    "                locals_list.append(vname)\n",
    "    # Programmkode als Klartext\n",
    "    code_str = \"\"\n",
    "    data = pou.find('.//ns:addData/ns:data[@name=\\\"http://www.3s-software.com/plcopenxml/interfaceasplaintext\\\"]', NS)\n",
    "    if data is not None:\n",
    "        code_el = data.find('.//InterfaceAsPlainText')\n",
    "        if code_el is not None and code_el.text:\n",
    "            code_str = code_el.text\n",
    "    pou_info[name] = {\"locals\": locals_list, \"code\": code_str}\n",
    "\n",
    "# 4) Mapping‑Einträge erweitern\n",
    "for entry in mapping:\n",
    "    info = pou_info.get(entry[\"Programm_Name\"])\n",
    "    if info:\n",
    "        # interne Variablen als temps\n",
    "        entry[\"temps\"] = [{\"name\": lv} for lv in info[\"locals\"]]\n",
    "        # Programmkode als eigener Schlüssel\n",
    "        entry[\"program_code\"] = info[\"code\"]\n",
    "\n",
    "# 5) Erweiterte JSON erneut speichern\n",
    "json_file.write_text(json.dumps(mapping, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(\"JSON um interne Variablen und Programmkode erweitert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b95cbc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON um interne Variablen und Programmkode erweitert.\n"
     ]
    }
   ],
   "source": [
    "# Zusätzliche Informationen aus export.xml ergänzen\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# 1) JSON erneut laden\n",
    "json_file = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\program_io_with_mapping.json\")\n",
    "mapping = json.loads(json_file.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# 2) export.xml parsen\n",
    "xml_file = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\export.xml\")\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "NS = {\"ns\": \"http://www.plcopen.org/xml/tc6_0200\",\n",
    "      \"html\": \"http://www.w3.org/1999/xhtml\"}\n",
    "\n",
    "# 3) interne Variablen und Programmkode je POU sammeln\n",
    "pou_info = {}  # {Name: {\"locals\": [...], \"code\": \"...\"}}\n",
    "for pou in root.findall(\".//ns:pou\", NS):\n",
    "    name = pou.attrib.get(\"name\")\n",
    "    # interne Variablen\n",
    "    locals_list = []\n",
    "    local_vars = pou.find('ns:interface/ns:localVars', NS)\n",
    "    if local_vars is not None:\n",
    "        for v in local_vars.findall('ns:variable', NS):\n",
    "            vname = v.attrib.get('name')\n",
    "            if vname:\n",
    "                locals_list.append(vname)\n",
    "    # Programmkode als Klartext\n",
    "    code_str = \"\"\n",
    "    data = pou.find('.//ns:addData/ns:data[@name=\"http://www.3s-software.com/plcopenxml/interfaceasplaintext\"]', NS)\n",
    "    if data is not None:\n",
    "        code_el = data.find('.//InterfaceAsPlainText')\n",
    "        if code_el is not None and code_el.text:\n",
    "            code_str = code_el.text\n",
    "    pou_info[name] = {\"locals\": locals_list, \"code\": code_str}\n",
    "\n",
    "# 4) Mapping‑Einträge erweitern\n",
    "for entry in mapping:\n",
    "    name = entry[\"Programm_Name\"]\n",
    "    info = pou_info.get(name)\n",
    "    if info:\n",
    "        # interne Variablen als temps oder eigener Schlüssel ergänzen\n",
    "        entry[\"temps\"] = [{\"name\": lv} for lv in info[\"locals\"]]\n",
    "        # Programmkode als eigener Schlüssel ergänzen\n",
    "        entry[\"program_code\"] = info[\"code\"]\n",
    "\n",
    "# 5) Erweiterte JSON erneut speichern\n",
    "json_file.write_text(json.dumps(mapping, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(\"JSON um interne Variablen und Programmkode erweitert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e3bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse abgeschlossen. Ergebnisse in C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\variable_traces.json\n"
     ]
    }
   ],
   "source": [
    "import json, xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Hilfsfunktionen ===\n",
    "def base_name(expr: str) -> str:\n",
    "    return expr.split(\".\")[-1] if expr else \"\"\n",
    "\n",
    "# === Daten laden ===\n",
    "json_path = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\program_io_with_mapping.json\")\n",
    "xml_path  = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\export.xml\")\n",
    "\n",
    "pou_map_data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "pou_map = {entry[\"Programm_Name\"]: entry for entry in pou_map_data}\n",
    "\n",
    "# Hardwarevariablen (xDI/udiDI, xDO/udiDO) aus der export.XML auslesen:contentReference[oaicite:0]{index=0}\n",
    "NS = {\"ns\":\"http://www.plcopen.org/xml/tc6_0200\",\"html\":\"http://www.w3.org/1999/xhtml\"}\n",
    "root = ET.parse(xml_path).getroot()\n",
    "var_doc = {}    # Variable -> physische Adresse\n",
    "hw_inputs = set()\n",
    "hw_outputs = set()\n",
    "for var in root.findall(\".//ns:variable\", NS):\n",
    "    name = var.attrib.get(\"name\")\n",
    "    doc = var.find(\".//html:xhtml\", NS)\n",
    "    if doc is not None and doc.text:\n",
    "        doc_text = doc.text.strip()\n",
    "        var_doc[name] = doc_text\n",
    "        if doc_text.startswith((\"xDI\",\"udiDI\")):\n",
    "            hw_inputs.add(name)\n",
    "        elif doc_text.startswith((\"xDO\",\"udiDO\")):\n",
    "            hw_outputs.add(name)\n",
    "\n",
    "# === Variablen‑Graph erzeugen ===\n",
    "# Knoten: Variablen-Basisname; Kanten: (Program, neues Basisname)\n",
    "var_graph = defaultdict(list)\n",
    "for entry in pou_map_data:\n",
    "    pname = entry[\"Programm_Name\"]\n",
    "    # externe Eingangs- und Ausgangsvariablen sammeln\n",
    "    in_bases  = [base_name(inp[\"external\"]) for inp in entry[\"inputs\"] if inp.get(\"external\")]\n",
    "    out_bases = [base_name(out[\"external\"]) for out in entry[\"outputs\"] if out.get(\"external\")]\n",
    "    for b_in in in_bases:\n",
    "        for b_out in out_bases:\n",
    "            var_graph[b_in].append((pname, b_out))\n",
    "\n",
    "# === Rekursives Tracing von Variablen zu Hardware ===\n",
    "def find_paths(start_base, visited_bases=None, depth=0):\n",
    "    \"\"\"Gibt für eine Variable (Basisname) alle Pfade (Programmkette und Variable) bis zur HW zurück.\"\"\"\n",
    "    if visited_bases is None:\n",
    "        visited_bases = set()\n",
    "    if start_base in visited_bases:\n",
    "        return []\n",
    "    visited_bases.add(start_base)\n",
    "\n",
    "    # direkter HW‑Treffer: keine weiteren Programme\n",
    "    if start_base in hw_outputs:\n",
    "        return [[]]\n",
    "\n",
    "    paths = []\n",
    "    for prog, new_base in var_graph.get(start_base, []):\n",
    "        for sub_path in find_paths(new_base, visited_bases.copy(), depth+1):\n",
    "            paths.append([(prog, new_base)] + sub_path)\n",
    "    return paths\n",
    "\n",
    "# === Programmausgabe: Pro Programm alle Outputs und Pfade ===\n",
    "trace = {}\n",
    "for pname, entry in pou_map.items():\n",
    "    prog_outputs = []\n",
    "    for out in entry[\"outputs\"]:\n",
    "        internal = out[\"internal\"]\n",
    "        ext      = out.get(\"external\")\n",
    "        if not ext:\n",
    "            continue\n",
    "        b = base_name(ext)\n",
    "        if b in hw_outputs:\n",
    "            prog_outputs.append({\n",
    "                \"internal\": internal,\n",
    "                \"external\": ext,\n",
    "                \"hardware\": True,\n",
    "                \"paths\": [[(pname, b), {\"hardware\": var_doc.get(b)}]]\n",
    "            })\n",
    "        else:\n",
    "            chains = []\n",
    "            for path in find_paths(b):\n",
    "                chain = [{\"program\": pname, \"variable\": b}]\n",
    "                for step_prog, step_base in path:\n",
    "                    chain.append({\"program\": step_prog, \"variable\": step_base})\n",
    "                if path:\n",
    "                    last_base = path[-1][1]\n",
    "                    chain.append({\"hardware\": var_doc.get(last_base)})\n",
    "                chains.append(chain)\n",
    "            prog_outputs.append({\n",
    "                \"internal\": internal,\n",
    "                \"external\": ext,\n",
    "                \"hardware\": False,\n",
    "                \"paths\": chains\n",
    "            })\n",
    "    trace[pname] = prog_outputs\n",
    "\n",
    "# Ergebnis als JSON speichern oder weiterverarbeiten\n",
    "out_file = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\variable_traces.json\")\n",
    "out_file.write_text(json.dumps(trace, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(f\"Analyse abgeschlossen. Ergebnisse in {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b1b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene POUs: ['HRL_SkillSet', 'MBS_SkillSet', 'SST_SkillSet', 'VSG_SkillSet', 'AutomaticColorDetection_nichtfertig', 'AxisControl_MultipleInputSensors', 'AxisControl_Encoder', 'CompressorControl', 'SecuringWorkpiece', 'StampingProcess', 'HardeningProcess', 'CuttingProcess', 'HRL_ControlOfOutputs', 'MBS_ControlOfOutputs', 'SST_ControlOfOutputs', 'VSG_ControlOfOutputs', 'HRL_RGB_InitialStateDrive', 'MBS_DmPD_InitialStateDrive', 'MBS_MR01_InitialStateDrive', 'MBS_MR02_InitialStateDrive', 'MBS_VSG_InitialStateDrive', 'SST_PD_InitialStateDrive', 'VSG_InitialStateDrive', 'FB_MyOpcUaMethod', 'JobMethode_Schablone', 'HRL_OperatingModes', 'MBS_OperatingModes', 'SST_OperatingModes', 'VSG_OperatingModes', 'HRL_RGB_AS_VerticalMoveEncoders', 'HRL_RGB_AS_HorizontalMoveSensors', 'HRL_CB_AS_HorizontalMoveSensors', 'HRL_RGB_AS_HorizontalMoveEncoders', 'MBS_MR01_AS_HardeningProcess', 'MBS_MR01_AS_SecuringProcess', 'MBS_VSG_AS_SuctionProcess', 'MBS_MR01_AS_HorizontalMoveSensors', 'MBS_DmPD_AS_RotationMove', 'MBS_VSG_AS_HorizontalMove', 'MBS_CB_AS_HorizontalMove', 'MBS_DmPD_AS_HorizontalMove', 'MBS_MR02_AS_CuttingProcess', 'MBS_VSG_AS_VerticalMove', 'SST_PD_AS_HorizontalMove', 'SST_CB_AS_HorizontalMove', 'SST_CS_AS_ColorDetection', 'VSG_AS_HorizontalMoveEncoder', 'VSG_AS_VerticalMoveEncoder', 'VSG_AS_RotationMoveEncoder', 'VSG_AS_CompressorControl']\n",
      "\n",
      "=== Verarbeite POU: HRL_SkillSet ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_SkillSet ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SST_SkillSet ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_SkillSet ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: AutomaticColorDetection_nichtfertig ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: AxisControl_MultipleInputSensors ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: AxisControl_Encoder ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: CompressorControl ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SecuringWorkpiece ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: StampingProcess ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HardeningProcess ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: CuttingProcess ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HRL_ControlOfOutputs ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_ControlOfOutputs ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SST_ControlOfOutputs ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_ControlOfOutputs ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HRL_RGB_InitialStateDrive ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_DmPD_InitialStateDrive ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_MR01_InitialStateDrive ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_MR02_InitialStateDrive ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_VSG_InitialStateDrive ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SST_PD_InitialStateDrive ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_InitialStateDrive ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: FB_MyOpcUaMethod ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Hinweis: variable_map ist leer, Datei wurde unverändert kopiert.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: JobMethode_Schablone ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HRL_OperatingModes ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_OperatingModes ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SST_OperatingModes ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_OperatingModes ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HRL_RGB_AS_VerticalMoveEncoders ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HRL_RGB_AS_HorizontalMoveSensors ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HRL_CB_AS_HorizontalMoveSensors ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: HRL_RGB_AS_HorizontalMoveEncoders ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_MR01_AS_HardeningProcess ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_MR01_AS_SecuringProcess ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_VSG_AS_SuctionProcess ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_MR01_AS_HorizontalMoveSensors ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_DmPD_AS_RotationMove ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_VSG_AS_HorizontalMove ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_CB_AS_HorizontalMove ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_DmPD_AS_HorizontalMove ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_MR02_AS_CuttingProcess ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: MBS_VSG_AS_VerticalMove ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SST_PD_AS_HorizontalMove ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SST_CB_AS_HorizontalMove ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: SST_CS_AS_ColorDetection ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_AS_HorizontalMoveEncoder ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_AS_VerticalMoveEncoder ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_AS_RotationMoveEncoder ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "=== Verarbeite POU: VSG_AS_CompressorControl ===\n",
      "Python-Code nach generated_code_1.py geschrieben.\n",
      "Umbenannte Datei nach generated_code_2.py geschrieben.\n",
      "Generated code has been written to  generated_code_3.py .\n",
      "Generated code has been written to generated_code_3.py.\n",
      "\n",
      "Fertig: Für alle POUs wurde der program_code gesetzt.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\MA_Python_Agent\\PyLC\")\n",
    "\n",
    "from PyLC1_Converter import parse_pou_blocks\n",
    "from PyLC2_Generator import generate_python_code\n",
    "from PyLC3_Rename import rename_variables\n",
    "from PyLC4_Cleanup import cleanup_code\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Pfade\n",
    "xml_path = r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\export.xml\"\n",
    "json_path = Path(r\"C:\\Users\\Alexander Verkhov\\OneDrive\\Dokumente\\MPA\\twincat2\\TwinCAT\\program_io_with_mapping.json\")\n",
    "\n",
    "# --- Alle POU-Namen aus der export.xml holen ---\n",
    "ns = {\"plcopen\": \"http://www.plcopen.org/xml/tc6_0200\"}\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "pou_names = [p.get(\"name\") for p in root.findall(\".//plcopen:pou\", ns)]\n",
    "\n",
    "print(\"Gefundene POUs:\", pou_names)\n",
    "\n",
    "# --- JSON einmal laden ---\n",
    "mapping = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# --- Für jeden POU: PyLC1–4 laufen lassen und program_code setzen ---\n",
    "for pou_name in pou_names:\n",
    "    print(f\"\\n=== Verarbeite POU: {pou_name} ===\")\n",
    "\n",
    "    # 1. Intermediate-Code für dieses POU erzeugen\n",
    "    parse_pou_blocks(\n",
    "        xml_path=xml_path,\n",
    "        output_path=\"generated_code_0.py\",\n",
    "        target_pou_name=pou_name,\n",
    "    )\n",
    "\n",
    "    # 2. Python-Code aus dem Intermediate-Code generieren\n",
    "    generate_python_code(\n",
    "        blocks_module_path=\"generated_code_0.py\",\n",
    "        output_path=\"generated_code_1.py\",\n",
    "    )\n",
    "\n",
    "    # 3. Variablennamen ersetzen\n",
    "    rename_variables(\n",
    "        input_code_path=\"generated_code_1.py\",\n",
    "        blocks_module_path=\"generated_code_0.py\",\n",
    "        output_path=\"generated_code_2.py\",\n",
    "    )\n",
    "\n",
    "    # 4. Redundante Argumente und Variablen bereinigen\n",
    "    cleanup_code(\n",
    "        input_code_path=\"generated_code_2.py\",\n",
    "        output_path=\"generated_code_3.py\",\n",
    "    )\n",
    "\n",
    "    # 5. Finalen Python-Code für dieses POU laden\n",
    "    with open(\"generated_code_3.py\", \"r\", encoding=\"utf-8\") as f:\n",
    "        python_code = f.read()\n",
    "\n",
    "    # 5a. Rückumwandlung der Platzhalter für die JSON-Ablage\n",
    "    #     (Python-Dateien selbst bleiben mit __DOT__, damit sie importierbar bleiben)\n",
    "    python_code_for_json = python_code.replace(\"__DOT__\", \".\")\n",
    "\n",
    "    # 6. Diesen Code in program_io_with_mapping.json beim passenden Programm speichern\n",
    "    for entry in mapping:\n",
    "        if entry.get(\"Programm_Name\") == pou_name:\n",
    "            entry[\"program_code\"] = python_code_for_json\n",
    "\n",
    "# 7. JSON zurückschreiben\n",
    "json_path.write_text(json.dumps(mapping, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"\\nFertig: Für alle POUs wurde der program_code gesetzt.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
