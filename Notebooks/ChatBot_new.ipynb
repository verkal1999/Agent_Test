{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef57a05",
   "metadata": {},
   "source": [
    "# PLC KG ChatBot (Single Notebook)\n",
    "\n",
    "Dieses Notebook baut einen **einzelnen** ChatBot auf deinem PLC Knowledge Graph (TTL/RDF) auf.\n",
    "\n",
    "Design-Ziele:\n",
    "- **Deterministisch wo möglich** (Tools für Call-Graph, Variable-Info, Trace, Similarity)\n",
    "- **LLM nur als Planner + Text2SPARQL-Fallback**\n",
    "- **Guardrails**: nur SELECT, LIMIT erzwingen, Code-Fences strippen\n",
    "- **Plan → Execute → Answer** Ablauf (debugbar)\n",
    "\n",
    "Referenzen / Best Practices:\n",
    "- Plan-and-Execute Agent Pattern (LangGraph)\n",
    "- SPARQL QA Chains & SPARQL Extraction Helper (LangChain)\n",
    "- Tool-Guardrails & Role-Isolation gegen Prompt-Injection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc85cf",
   "metadata": {},
   "source": [
    "## 0) Installation (optional)\n",
    "Wenn du lokal etwas vermisst, installiere hier die Dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebff1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (7.5.0)\n",
      "Requirement already satisfied: pandas in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: ipywidgets in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (8.1.8)\n",
      "Requirement already satisfied: langchain-core in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (1.2.9)\n",
      "Requirement already satisfied: langchain-openai in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: langchain-community in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: pydantic in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (2.12.5)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from rdflib) (3.2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core) (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.2.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core) (0.13.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-openai) (2.8.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2026.1.15)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.6.3)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: colorama in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: faiss-cpu in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: langchain-openai in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (1.1.7)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from faiss-cpu) (2.3.5)\n",
      "Requirement already satisfied: packaging in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-openai) (1.2.9)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-openai) (2.8.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2026.1.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.6.3)\n",
      "Requirement already satisfied: colorama in d:\\MA_Python_Agent\\.venv311\\Lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Optional: einmalig ausführen (lokal)\n",
    "%pip install -U rdflib pandas ipywidgets langchain-core langchain-openai langchain-community pydantic\n",
    "%pip install faiss-cpu langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0763f7",
   "metadata": {},
   "source": [
    "## 1) Konfiguration\n",
    "Passe die Pfade und Modelle an. Der Code versucht automatisch, eine TTL im selben Ordner oder unter /mnt/data zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854b9f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTL_PATH = D:\\MA_Python_Agent\\MSRGuard_Anpassung\\KGs\\TestEvents.ttl\n",
      "INDEX_PATH = D:\\MA_Python_Agent\\MSRGuard_Anpassung\\KGs\\ChatBotRoutinen\\TestEvents_routine_index.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === Pfad zur TTL-Datei ===\n",
    "# 1) Lokal: setze hier deinen absoluten Pfad.\n",
    "TTL_PATH = r\"D:\\MA_Python_Agent\\MSRGuard_Anpassung\\KGs\\TestEvents.ttl\"\n",
    "filename = \"TestEvents.ttl\"\n",
    "\n",
    "# 2) Autodetect (z.B. Sandbox)\n",
    "\n",
    "print(\"TTL_PATH =\", TTL_PATH)\n",
    "\n",
    "# === Index-Datei (Similarity / Routine Index) ===\n",
    "index_name = filename.replace(\".ttl\", \"_routine_index.json\")\n",
    "INDEX_DIR = Path(r\"D:\\MA_Python_Agent\\MSRGuard_Anpassung\\KGs\\ChatBotRoutinen\")\n",
    "INDEX_PATH = str(INDEX_DIR / index_name)\n",
    "print(\"INDEX_PATH =\", INDEX_PATH)\n",
    "\n",
    "# === LLM Backend ===\n",
    "# \"openai\" (via langchain_openai). Du kannst später \"gemini\" ergänzen.\n",
    "LLM_BACKEND = \"openai\"\n",
    "\n",
    "# OpenAI (LangChain) Settings\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "OPENAI_TEMPERATURE = 0\n",
    "\n",
    "# Limits\n",
    "MAX_SPARQL_ROWS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03f8c9",
   "metadata": {},
   "source": [
    "## 2) Graph laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d945fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph geladen\n",
      "Triples: 2256\n",
      "Namespaces (Auszug): [('brick', rdflib.term.URIRef('https://brickschema.org/schema/Brick#')), ('csvw', rdflib.term.URIRef('http://www.w3.org/ns/csvw#')), ('dc', rdflib.term.URIRef('http://purl.org/dc/elements/1.1/')), ('dcat', rdflib.term.URIRef('http://www.w3.org/ns/dcat#')), ('dcmitype', rdflib.term.URIRef('http://purl.org/dc/dcmitype/')), ('dcterms', rdflib.term.URIRef('http://purl.org/dc/terms/')), ('dcam', rdflib.term.URIRef('http://purl.org/dc/dcam/')), ('doap', rdflib.term.URIRef('http://usefulinc.com/ns/doap#')), ('foaf', rdflib.term.URIRef('http://xmlns.com/foaf/0.1/')), ('geo', rdflib.term.URIRef('http://www.opengis.net/ont/geosparql#'))]\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "g = Graph()\n",
    "g.parse(TTL_PATH, format=\"turtle\")\n",
    "\n",
    "print(\"✅ Graph geladen\")\n",
    "print(\"Triples:\", len(g))\n",
    "print(\"Namespaces (Auszug):\", list(g.namespaces())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f3576",
   "metadata": {},
   "source": [
    "## 3) Schema Card (kompakte KG-Übersicht)\n",
    "Diese Übersicht geht in Planner und Text2SPARQL Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e2dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP CLASSES (rdf:type):\n",
      "  - ag:class_Variable: 147\n",
      "  - ag:class_Port: 81\n",
      "  - ag:class_ParameterAssignment: 56\n",
      "  - ag:class_FBInstance: 51\n",
      "  - owl:NamedIndividual: 50\n",
      "  - owl:DatatypeProperty: 35\n",
      "  - owl:ObjectProperty: 31\n",
      "  - ag:class_POUCall: 27\n",
      "  - ag:class_SignalSource: 23\n",
      "  - owl:Class: 19\n",
      "  - ag:class_FBType: 17\n",
      "  - ag:class_PortInstance: 16\n",
      "  - ag:class_StandardFBType: 10\n",
      "  - ag:class_SourceLiteral: 7\n",
      "  - ag:class_CustomFBType: 7\n",
      "\n",
      "TOP PROPERTIES:\n",
      "  - rdf:type: 587\n",
      "  - dp:hasVariableName: 170\n",
      "  - dp:hasVariableType: 147\n",
      "  - op:usesVariable: 124\n",
      "  - op:hasInternalVariable: 124\n",
      "  - op:hasPort: 81\n",
      "  - dp:hasPortDirection: 81\n",
      "  - dp:hasPortType: 81\n",
      "  - dp:hasPortName: 81\n",
      "  - op:hasAssignment: 56\n",
      "  - op:assignsFrom: 56\n",
      "  - rdfs:domain: 55\n",
      "  - rdfs:range: 55\n",
      "  - op:assignsToPort: 52\n",
      "  - op:isInstanceOfFBType: 51\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from rdflib.namespace import RDF\n",
    "\n",
    "def schema_card(graph: Graph, top_n: int = 15) -> str:\n",
    "    pred_counts = Counter()\n",
    "    type_counts = Counter()\n",
    "\n",
    "    for s, p, o in graph:\n",
    "        try:\n",
    "            pred_counts[graph.qname(p)] += 1\n",
    "        except Exception:\n",
    "            pred_counts[str(p)] += 1\n",
    "\n",
    "        if p == RDF.type:\n",
    "            try:\n",
    "                type_counts[graph.qname(o)] += 1\n",
    "            except Exception:\n",
    "                type_counts[str(o)] += 1\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"TOP CLASSES (rdf:type):\")\n",
    "    for k, v in type_counts.most_common(top_n):\n",
    "        lines.append(f\"  - {k}: {v}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"TOP PROPERTIES:\")\n",
    "    for k, v in pred_counts.most_common(top_n):\n",
    "        lines.append(f\"  - {k}: {v}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "SCHEMA_CARD = schema_card(g, top_n=15)\n",
    "print(SCHEMA_CARD[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468543f7",
   "metadata": {},
   "source": [
    "## 4) Tools mit SPARQL Helper (Guardrails)\n",
    "- nur SELECT\n",
    "- blockt UPDATE/Service\n",
    "- erzwingt LIMIT\n",
    "- Ergebnisse als Liste von Dicts\n",
    "- Tools beantworten typische Fragen\n",
    "\n",
    "Wenn ein LLM SPARQL in Codeblöcke packt, extrahieren wir es robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395fda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inspect\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "from rdflib import Graph\n",
    "\n",
    "# ==========================================\n",
    "# 1. SPARQL HELPER & GUARDRAILS\n",
    "# ==========================================\n",
    "\n",
    "DEFAULT_PREFIXES = \"\"\"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX ag:  <http://www.semanticweb.org/AgentProgramParams/>\n",
    "PREFIX dp:  <http://www.semanticweb.org/AgentProgramParams/dp_>\n",
    "PREFIX op:  <http://www.semanticweb.org/AgentProgramParams/op_>\n",
    "\"\"\"\n",
    "\n",
    "def _normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def enforce_select_only(query: str, max_limit: int = 200) -> str:\n",
    "    \"\"\"Verhindert gefährliche Operationen und erzwingt LIMIT.\"\"\"\n",
    "    q = query.strip()\n",
    "    q_u = _normalize_ws(q).upper()\n",
    "\n",
    "    if not (q_u.startswith(\"PREFIX\") or q_u.startswith(\"SELECT\")):\n",
    "        raise ValueError(\"Only SELECT queries are allowed (optionally with PREFIX).\")\n",
    "\n",
    "    forbidden = [\n",
    "        \"INSERT\",\"DELETE\",\"LOAD\",\"CLEAR\",\"CREATE\",\"DROP\",\"MOVE\",\"COPY\",\"ADD\",\n",
    "        \"SERVICE\",\"WITH\",\"USING\",\"GRAPH\"\n",
    "    ]\n",
    "    for kw in forbidden:\n",
    "        if re.search(rf\"\\b{kw}\\b\", q_u):\n",
    "            raise ValueError(f\"Forbidden SPARQL keyword detected: {kw}\")\n",
    "\n",
    "    m = re.search(r\"\\bLIMIT\\s+(\\d+)\\b\", q_u)\n",
    "    if m:\n",
    "        lim = int(m.group(1))\n",
    "        if lim > max_limit:\n",
    "            q = re.sub(r\"(?i)\\bLIMIT\\s+\\d+\\b\", f\"LIMIT {max_limit}\", q)\n",
    "    else:\n",
    "        q = q.rstrip() + f\"\\nLIMIT {max_limit}\\n\"\n",
    "    return q\n",
    "\n",
    "def strip_code_fences(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    t = re.sub(r\"^```[a-zA-Z]*\\s*\", \"\", t)\n",
    "    t = re.sub(r\"\\s*```$\", \"\", t)\n",
    "    return t.strip()\n",
    "\n",
    "# Versuch, LangChain Helper zu laden (optional)\n",
    "try:\n",
    "    from langchain_community.chains.graph_qa.neptune_sparql import extract_sparql as lc_extract_sparql\n",
    "except Exception:\n",
    "    lc_extract_sparql = None\n",
    "\n",
    "def extract_sparql_from_llm(text: str) -> str:\n",
    "    \"\"\"Extrahiert reinen SPARQL Code aus einer LLM Antwort.\"\"\"\n",
    "    if lc_extract_sparql is not None:\n",
    "        try:\n",
    "            return lc_extract_sparql(text).strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "    t = strip_code_fences(text)\n",
    "    m = re.search(r\"(SELECT\\s+.*)\", t, flags=re.IGNORECASE | re.DOTALL)\n",
    "    return (m.group(1).strip() if m else t)\n",
    "\n",
    "def sparql_select_raw(query: str, max_rows: int = 200) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Führt die Query auf dem globalen Graph 'g' aus.\n",
    "    Fügt Prefixes hinzu und formatiert das Ergebnis als Liste von Dicts.\n",
    "    \"\"\"\n",
    "    # Zugriff auf globale Variable 'g' (rdflib.Graph)\n",
    "    if 'g' not in globals():\n",
    "        raise RuntimeError(\"Global graph 'g' not found via globals().\")\n",
    "    \n",
    "    q = query.strip()\n",
    "    if \"PREFIX\" not in q.upper():\n",
    "        q = DEFAULT_PREFIXES + \"\\n\" + q\n",
    "    \n",
    "    # Guardrails anwenden\n",
    "    q = enforce_select_only(q, max_limit=max_rows)\n",
    "\n",
    "    res = g.query(q)\n",
    "    vars_ = [str(v) for v in res.vars]\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for row in res:\n",
    "        item = {}\n",
    "        for i, v in enumerate(vars_):\n",
    "            val = row[i]\n",
    "            item[v] = None if val is None else str(val)\n",
    "        out.append(item)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. AGENT TOOLS (COMMAND PATTERN)\n",
    "# ==========================================\n",
    "\n",
    "class BaseAgentTool(ABC):\n",
    "    \"\"\"Abstrakte Basisklasse für alle Tools.\"\"\"\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    usage_guide: str = \"\"\n",
    "\n",
    "    def get_prompt_signature(self) -> str:\n",
    "        sig = inspect.signature(self.run)\n",
    "        params = [\n",
    "            f\"{k}\" \n",
    "            for k, v in sig.parameters.items() \n",
    "            if k != \"self\" and v.kind != inspect.Parameter.VAR_KEYWORD\n",
    "        ]\n",
    "        return f\"{self.name}({', '.join(params)})\"\n",
    "\n",
    "    def get_documentation(self) -> str:\n",
    "        return (\n",
    "            f\"- {self.get_prompt_signature()}\\n\"\n",
    "            f\"  Beschreibung: {self.description}\\n\"\n",
    "            f\"  Wann nutzen: {self.usage_guide}\\n\"\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self, **kwargs) -> Any:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ListProgramsTool(BaseAgentTool):\n",
    "    name = \"list_programs\"\n",
    "    description = \"Listet alle verfügbaren Programme im Projekt auf.\"\n",
    "    usage_guide = \"Wenn der User fragt 'Welche Programme gibt es?' oder einen Einstiegspunkt sucht.\"\n",
    "\n",
    "    def run(self, **kwargs) -> List[Dict[str, Any]]:\n",
    "        q = \"\"\"\n",
    "        SELECT ?programName WHERE {\n",
    "          ?program rdf:type ag:class_Program ;\n",
    "                   dp:hasProgramName ?programName .\n",
    "        } ORDER BY ?programName\n",
    "        \"\"\"\n",
    "        return sparql_select_raw(q)\n",
    "\n",
    "\n",
    "class CalledPousTool(BaseAgentTool):\n",
    "    name = \"called_pous\"\n",
    "    description = \"Zeigt alle POUs, die von einem Programm aufgerufen werden.\"\n",
    "    usage_guide = \"Bei Fragen nach Call-Graph, Struktur, 'Wer ruft wen auf?'.\"\n",
    "\n",
    "    def run(self, program_name: str, **kwargs) -> List[Dict[str, Any]]:\n",
    "        q = f\"\"\"\n",
    "        SELECT DISTINCT ?calleeName WHERE {{\n",
    "          ?program rdf:type ag:class_Program ;\n",
    "                   dp:hasProgramName \"{program_name}\" ;\n",
    "                   op:containsPOUCall ?call .\n",
    "          ?call op:callsPOU ?callee .\n",
    "          OPTIONAL {{ ?callee dp:hasPOUName ?calleeName }}\n",
    "        }} ORDER BY ?calleeName\n",
    "        \"\"\"\n",
    "        return sparql_select_raw(q)\n",
    "\n",
    "\n",
    "class PouCodeTool(BaseAgentTool):\n",
    "    name = \"pou_code\"\n",
    "    description = \"Holt ST-Code, Sprache und Report einer POU.\"\n",
    "    usage_guide = \"Wenn User nach 'Code', 'Implementierung' oder 'Inhalt' fragt.\"\n",
    "\n",
    "    def run(self, pou_name: str, **kwargs) -> List[Dict[str, Any]]:\n",
    "        q = f\"\"\"\n",
    "        SELECT ?lang ?code ?report WHERE {{\n",
    "          ?pou dp:hasPOUName \"{pou_name}\" .\n",
    "          OPTIONAL {{ ?pou dp:hasPOULanguage ?lang }}\n",
    "          OPTIONAL {{ ?pou dp:hasPOUCode ?code }}\n",
    "          OPTIONAL {{ ?pou dp:hasConsistencyReport ?report }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return sparql_select_raw(q)\n",
    "\n",
    "\n",
    "class SearchVariablesTool(BaseAgentTool):\n",
    "    name = \"search_variables\"\n",
    "    description = \"Sucht Variablen nach Name (Teilstring).\"\n",
    "    usage_guide = \"Fragen nach 'Variable', 'Adresse', 'I/O' oder Signalnamen.\"\n",
    "\n",
    "    def run(self, name_contains: str, **kwargs) -> List[Dict[str, Any]]:\n",
    "        needle = name_contains.replace('\"', '\\\\\"')\n",
    "        q = f\"\"\"\n",
    "        SELECT DISTINCT ?name ?type ?addr WHERE {{\n",
    "          ?var rdf:type ag:class_Variable ;\n",
    "               dp:hasVariableName ?name ;\n",
    "               dp:hasVariableType ?type .\n",
    "          FILTER(CONTAINS(LCASE(STR(?name)), LCASE(\"{needle}\")))\n",
    "          OPTIONAL {{ ?var dp:hasHardwareAddress ?addr }}\n",
    "        }} ORDER BY ?name\n",
    "        \"\"\"\n",
    "        return sparql_select_raw(q)\n",
    "\n",
    "\n",
    "class VariableTraceTool(BaseAgentTool):\n",
    "    name = \"variable_trace\"\n",
    "    description = \"Analysiert Schreib-/Lesezugriffe auf Variablen (Data Flow).\"\n",
    "    usage_guide = \"Fragen wie 'Woher kommt Signal X?', 'Wer nutzt Variable Y?'.\"\n",
    "\n",
    "    def run(self, name_contains: str, **kwargs) -> List[Dict[str, Any]]:\n",
    "        needle = name_contains.replace('\"', '\\\\\"')\n",
    "        q = f\"\"\"\n",
    "        SELECT DISTINCT ?varName ?exprText ?calleeName WHERE {{\n",
    "          ?var rdf:type ag:class_Variable ;\n",
    "               dp:hasVariableName ?varName .\n",
    "          FILTER(CONTAINS(LCASE(STR(?varName)), LCASE(\"{needle}\")))\n",
    "          \n",
    "          OPTIONAL {{\n",
    "            ?expr rdf:type ag:class_Expression ;\n",
    "                  dp:hasExpressionText ?exprText ;\n",
    "                  op:isExpressionCreatedBy ?var .\n",
    "            OPTIONAL {{\n",
    "              ?assign rdf:type ag:class_ParameterAssignment ;\n",
    "                      op:assignsFrom ?expr .\n",
    "              OPTIONAL {{\n",
    "                ?pouCall rdf:type ag:class_POUCall ;\n",
    "                         op:hasAssignment ?assign ;\n",
    "                         op:callsPOU ?callee .\n",
    "                OPTIONAL {{ ?callee dp:hasPOUName ?calleeName }}\n",
    "              }}\n",
    "            }}\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return sparql_select_raw(q)\n",
    "\n",
    "class PouCallersTool(BaseAgentTool):\n",
    "    name = \"pou_callers\"\n",
    "    description = \"Findet heraus, von welchen Programmen oder FBs eine POU aufgerufen wird (Reverse Call Graph).\"\n",
    "    # Hier fügen wir 'Was macht...' hinzu, damit der Planner anspringt\n",
    "    usage_guide = \"Nutzen bei Fragen wie 'Wer nutzt X?', 'Wo wird X verwendet?' oder allgemein 'Was macht X?' (um den Kontext zu zeigen).\"\n",
    "\n",
    "    def run(self, pou_name: str, **kwargs) -> List[Dict[str, Any]]:\n",
    "        # Wir suchen alle POUs (?caller), die einen Call (?call) beinhalten,\n",
    "        # der auf unsere Ziel-POU (?target) zeigt.\n",
    "        q = f\"\"\"\n",
    "        SELECT DISTINCT ?callerName WHERE {{\n",
    "          ?targetPou dp:hasPOUName \"{pou_name}\" .\n",
    "          ?call op:callsPOU ?targetPou .\n",
    "          \n",
    "          ?caller op:containsPOUCall ?call ;\n",
    "                  dp:hasPOUName ?callerName .\n",
    "        }} ORDER BY ?callerName\n",
    "        \"\"\"\n",
    "        return sparql_select_raw(q)\n",
    "    \n",
    "class ExceptionAnalysisTool(BaseAgentTool):\n",
    "    name = \"exception_prep\"\n",
    "    description = \"Analysiert einen Snapshot gegen Routine-Signaturen.\"\n",
    "    usage_guide = \"Bei konkreten Sensorwerten oder 'Fehlerbild'.\"\n",
    "\n",
    "    def __init__(self, kg_store, index):\n",
    "        self.kg = kg_store\n",
    "        self.index = index\n",
    "\n",
    "    def run(self, program_name: str, snapshot: Dict[str, Any], top_k: int = 5, **kwargs) -> Dict[str, Any]:\n",
    "        # Hinweis: SignatureExtractor/SensorSnapshot Klassen müssen im Notebook definiert sein\n",
    "        extractor = SignatureExtractor(self.kg)\n",
    "        try:\n",
    "            sig = extractor.extract_signature(program_name)\n",
    "        except ValueError as e:\n",
    "            return {\"error\": str(e)}\n",
    "            \n",
    "        snap = SensorSnapshot(program_name=program_name, sensor_values=snapshot)\n",
    "        check_map = classify_checkable_sensors(snap, sig)\n",
    "        similar = self.index.find_similar(sig, top_k=top_k)\n",
    "        \n",
    "        return {\n",
    "            \"signature\": sig.as_dict(),\n",
    "            \"checkable\": check_map,\n",
    "            \"similar\": similar,\n",
    "        }\n",
    "\n",
    "\n",
    "class Text2SparqlTool(BaseAgentTool):\n",
    "    name = \"text2sparql_select\"\n",
    "    description = \"Generiert und führt SPARQL SELECT aus (Fallback).\"\n",
    "    usage_guide = \"NUR nutzen, wenn kein anderes Tool passt.\"\n",
    "\n",
    "    def __init__(self, llm_invoke_fn: Callable, schema_card_text: str):\n",
    "        self.llm_invoke = llm_invoke_fn\n",
    "        self.schema_card = schema_card_text\n",
    "\n",
    "    def run(self, question: str, max_rows: int = 50, **kwargs) -> Dict[str, Any]:\n",
    "        system_prompt = f\"\"\"\n",
    "        Du bist ein SPARQL-Generator.\n",
    "        Regeln: Nur SELECT, Prefixes nutzen (rdf, ag, dp, op).\n",
    "        Schema:\n",
    "        {self.schema_card}\n",
    "        \"\"\"\n",
    "        raw = self.llm_invoke(system_prompt, question)\n",
    "        # Hier nutzen wir jetzt deine Helper-Funktionen:\n",
    "        q = extract_sparql_from_llm(raw)\n",
    "        rows = sparql_select_raw(q, max_rows=max_rows)\n",
    "        return {\"sparql\": q, \"rows\": rows}\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. REGISTRY & SETUP\n",
    "# ==========================================\n",
    "\n",
    "class ToolRegistry:\n",
    "    def __init__(self):\n",
    "        self._tools: Dict[str, BaseAgentTool] = {}\n",
    "\n",
    "    def register(self, tool: BaseAgentTool):\n",
    "        self._tools[tool.name] = tool\n",
    "\n",
    "    def get_system_prompt_part(self) -> str:\n",
    "        parts = [t.get_documentation() for t in self._tools.values()]\n",
    "        return \"Verfügbare Tools:\\n\" + \"\".join(parts)\n",
    "\n",
    "    def execute(self, tool_name: str, args: Dict[str, Any]) -> Any:\n",
    "        tool = self._tools.get(tool_name)\n",
    "        if not tool:\n",
    "            return {\"error\": f\"Tool '{tool_name}' not found.\"}\n",
    "        try:\n",
    "            return tool.run(**args)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error in '{tool_name}': {e}\"}\n",
    "\n",
    "# Init\n",
    "registry = ToolRegistry()\n",
    "\n",
    "# Komplexe Tools (benötigen Objekte aus vorherigen Zellen)\n",
    "if 'kg' in globals() and 'routine_index' in globals():\n",
    "    registry.register(ExceptionAnalysisTool(kg, routine_index))\n",
    "\n",
    "if 'llm_invoke' in globals() and 'SCHEMA_CARD' in globals():\n",
    "    registry.register(Text2SparqlTool(llm_invoke, SCHEMA_CARD))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec1deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Set, Tuple\n",
    "from rdflib import URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF\n",
    "\n",
    "AG = Namespace(\"http://www.semanticweb.org/AgentProgramParams/\")\n",
    "DP = Namespace(\"http://www.semanticweb.org/AgentProgramParams/dp_\")\n",
    "OP = Namespace(\"http://www.semanticweb.org/AgentProgramParams/op_\")\n",
    "\n",
    "@dataclass\n",
    "class SensorSnapshot:\n",
    "    program_name: str\n",
    "    sensor_values: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class RoutineSignature:\n",
    "    pou_name: str\n",
    "    reachable_pous: List[str]\n",
    "    called_pou_names: List[str]\n",
    "    used_variable_names: List[str]\n",
    "    hardware_addresses: List[str]\n",
    "    port_names: List[str]\n",
    "\n",
    "    def as_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"pou_name\": self.pou_name,\n",
    "            \"reachable_pous\": self.reachable_pous,\n",
    "            \"called_pou_names\": self.called_pou_names,\n",
    "            \"used_variable_names\": self.used_variable_names,\n",
    "            \"hardware_addresses\": self.hardware_addresses,\n",
    "            \"port_names\": self.port_names,\n",
    "        }\n",
    "\n",
    "class KGStore:\n",
    "    def __init__(self, graph: Graph):\n",
    "        self.g = graph\n",
    "        self._pou_by_name: Dict[str, URIRef] = {}\n",
    "        self._build_cache()\n",
    "\n",
    "    def _build_cache(self) -> None:\n",
    "        for pou, _, name in self.g.triples((None, DP.hasPOUName, None)):\n",
    "            if isinstance(name, Literal):\n",
    "                self._pou_by_name[str(name)] = pou\n",
    "\n",
    "    def pou_uri_by_name(self, pou_name: str) -> Optional[URIRef]:\n",
    "        return self._pou_by_name.get(pou_name)\n",
    "\n",
    "    def pou_name(self, pou_uri: URIRef) -> str:\n",
    "        v = self.g.value(pou_uri, DP.hasPOUName)\n",
    "        return str(v) if v else str(pou_uri)\n",
    "\n",
    "    def get_reachable_pous(self, root_pou_uri: URIRef) -> Set[URIRef]:\n",
    "        visited: Set[URIRef] = set()\n",
    "        queue: List[URIRef] = [root_pou_uri]\n",
    "        while queue:\n",
    "            cur = queue.pop(0)\n",
    "            if cur in visited:\n",
    "                continue\n",
    "            visited.add(cur)\n",
    "            for call in self.g.objects(cur, OP.containsPOUCall):\n",
    "                for called in self.g.objects(call, OP.callsPOU):\n",
    "                    if isinstance(called, URIRef) and called not in visited:\n",
    "                        queue.append(called)\n",
    "        return visited\n",
    "\n",
    "    def get_called_pous(self, pou_uri: URIRef) -> Set[URIRef]:\n",
    "        called: Set[URIRef] = set()\n",
    "        for call in self.g.objects(pou_uri, OP.containsPOUCall):\n",
    "            for target in self.g.objects(call, OP.callsPOU):\n",
    "                if isinstance(target, URIRef):\n",
    "                    called.add(target)\n",
    "        return called\n",
    "\n",
    "    def get_used_variables(self, pou_uri: URIRef) -> Set[URIRef]:\n",
    "        vars_: Set[URIRef] = set()\n",
    "        for v in self.g.objects(pou_uri, OP.usesVariable):\n",
    "            if isinstance(v, URIRef):\n",
    "                vars_.add(v)\n",
    "        for v in self.g.objects(pou_uri, OP.hasInternalVariable):\n",
    "            if isinstance(v, URIRef):\n",
    "                vars_.add(v)\n",
    "        return vars_\n",
    "\n",
    "    def get_variable_names(self, var_uri: URIRef) -> Set[str]:\n",
    "        names: Set[str] = set()\n",
    "        for _, _, name in self.g.triples((var_uri, DP.hasVariableName, None)):\n",
    "            if isinstance(name, Literal):\n",
    "                names.add(str(name))\n",
    "        return names\n",
    "\n",
    "    def get_hardware_address(self, var_uri: URIRef) -> Optional[str]:\n",
    "        v = self.g.value(var_uri, DP.hasHardwareAddress)\n",
    "        return str(v) if v else None\n",
    "\n",
    "    def get_ports_of_pou(self, pou_uri: URIRef) -> Set[URIRef]:\n",
    "        ports: Set[URIRef] = set()\n",
    "        for p in self.g.objects(pou_uri, OP.hasPort):\n",
    "            if isinstance(p, URIRef):\n",
    "                ports.add(p)\n",
    "        return ports\n",
    "\n",
    "    def get_port_name(self, port_uri: URIRef) -> str:\n",
    "        v = self.g.value(port_uri, DP.hasPortName)\n",
    "        return str(v) if v else \"\"\n",
    "\n",
    "kg = KGStore(g)\n",
    "\n",
    "def tool_list_programs() -> List[Dict[str, Any]]:\n",
    "    q = \"\"\"\n",
    "    SELECT ?programName WHERE {\n",
    "      ?program rdf:type ag:class_Program ;\n",
    "               dp:hasProgramName ?programName .\n",
    "    } ORDER BY ?programName\n",
    "    \"\"\"\n",
    "    return sparql_select_raw(q, max_rows=MAX_SPARQL_ROWS)\n",
    "\n",
    "def tool_get_program_overview(program_name: str) -> List[Dict[str, Any]]:\n",
    "    q = f\"\"\"\n",
    "    SELECT ?report WHERE {{\n",
    "      ?program rdf:type ag:class_Program ;\n",
    "               dp:hasProgramName \\\"{program_name}\\\" .\n",
    "      OPTIONAL {{ ?program dp:hasConsistencyReport ?report }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return sparql_select_raw(q, max_rows=MAX_SPARQL_ROWS)\n",
    "\n",
    "def tool_get_called_pous(program_name: str) -> List[Dict[str, Any]]:\n",
    "    q = f\"\"\"\n",
    "    SELECT DISTINCT ?calleeName WHERE {{\n",
    "      ?program rdf:type ag:class_Program ;\n",
    "               dp:hasProgramName \\\"{program_name}\\\" ;\n",
    "               op:containsPOUCall ?call .\n",
    "      ?call op:callsPOU ?callee .\n",
    "      OPTIONAL {{ ?callee dp:hasPOUName ?calleeName }}\n",
    "    }} ORDER BY ?calleeName\n",
    "    \"\"\"\n",
    "    return sparql_select_raw(q, max_rows=MAX_SPARQL_ROWS)\n",
    "\n",
    "def tool_get_pou_code(pou_name: str) -> List[Dict[str, Any]]:\n",
    "    q = f\"\"\"\n",
    "    SELECT ?lang ?code ?report WHERE {{\n",
    "      ?pou dp:hasPOUName \\\"{pou_name}\\\" .\n",
    "      OPTIONAL {{ ?pou dp:hasPOULanguage ?lang }}\n",
    "      OPTIONAL {{ ?pou dp:hasPOUCode ?code }}\n",
    "      OPTIONAL {{ ?pou dp:hasConsistencyReport ?report }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return sparql_select_raw(q, max_rows=MAX_SPARQL_ROWS)\n",
    "\n",
    "def tool_search_variables(name_contains: str) -> List[Dict[str, Any]]:\n",
    "    needle = name_contains.replace('\"', '\\\\\"')\n",
    "    q = f\"\"\"\n",
    "    SELECT DISTINCT ?name ?type ?addr WHERE {{\n",
    "      ?var rdf:type ag:class_Variable ;\n",
    "           dp:hasVariableName ?name ;\n",
    "           dp:hasVariableType ?type .\n",
    "      FILTER(CONTAINS(LCASE(STR(?name)), LCASE(\\\"{needle}\\\")))\n",
    "      OPTIONAL {{ ?var dp:hasHardwareAddress ?addr }}\n",
    "    }} ORDER BY ?name\n",
    "    \"\"\"\n",
    "    return sparql_select_raw(q, max_rows=MAX_SPARQL_ROWS)\n",
    "\n",
    "def tool_get_variable_trace(name_contains: str) -> List[Dict[str, Any]]:\n",
    "    needle = name_contains.replace('\"', '\\\\\"')\n",
    "    q = f\"\"\"\n",
    "    SELECT DISTINCT ?varName ?exprText ?calleeName WHERE {{\n",
    "      ?var rdf:type ag:class_Variable ;\n",
    "           dp:hasVariableName ?varName .\n",
    "      FILTER(CONTAINS(LCASE(STR(?varName)), LCASE(\\\"{needle}\\\")))\n",
    "\n",
    "      OPTIONAL {{\n",
    "        ?expr rdf:type ag:class_Expression ;\n",
    "              dp:hasExpressionText ?exprText ;\n",
    "              op:isExpressionCreatedBy ?var .\n",
    "        OPTIONAL {{\n",
    "          ?assign rdf:type ag:class_ParameterAssignment ;\n",
    "                  op:assignsFrom ?expr .\n",
    "          OPTIONAL {{\n",
    "            ?pouCall rdf:type ag:class_POUCall ;\n",
    "                     op:hasAssignment ?assign ;\n",
    "                     op:callsPOU ?callee .\n",
    "            OPTIONAL {{ ?callee dp:hasPOUName ?calleeName }}\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return sparql_select_raw(q, max_rows=MAX_SPARQL_ROWS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43009112",
   "metadata": {},
   "source": [
    "## 6) Routine-Signaturen + Similarity Index\n",
    "Speichert Signaturen in einer JSON-Datei neben der TTL, damit Similarity Checks schnell sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c37ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RoutineIndex geladen: D:\\MA_Python_Agent\\MSRGuard_Anpassung\\KGs\\ChatBotRoutinen\\TestEvents_routine_index.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def jaccard(a: Set[str], b: Set[str]) -> float:\n",
    "    if not a and not b:\n",
    "        return 0.0\n",
    "    inter = len(a & b)\n",
    "    union = len(a | b)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "class SignatureExtractor:\n",
    "    def __init__(self, kg: KGStore):\n",
    "        self.kg = kg\n",
    "\n",
    "    def extract_signature(self, pou_name: str) -> RoutineSignature:\n",
    "        pou_uri = self.kg.pou_uri_by_name(pou_name)\n",
    "        if pou_uri is None:\n",
    "            raise ValueError(f\"POU '{pou_name}' not found in KG.\")\n",
    "\n",
    "        reachable = self.kg.get_reachable_pous(pou_uri)\n",
    "\n",
    "        reachable_names: Set[str] = set()\n",
    "        called_names: Set[str] = set()\n",
    "        used_var_names: Set[str] = set()\n",
    "        hw_addrs: Set[str] = set()\n",
    "        port_names: Set[str] = set()\n",
    "\n",
    "        for rp in reachable:\n",
    "            reachable_names.add(self.kg.pou_name(rp))\n",
    "            for callee in self.kg.get_called_pous(rp):\n",
    "                called_names.add(self.kg.pou_name(callee))\n",
    "            for var in self.kg.get_used_variables(rp):\n",
    "                used_var_names |= self.kg.get_variable_names(var)\n",
    "                ha = self.kg.get_hardware_address(var)\n",
    "                if ha:\n",
    "                    hw_addrs.add(ha)\n",
    "            for port in self.kg.get_ports_of_pou(rp):\n",
    "                pn = self.kg.get_port_name(port)\n",
    "                if pn:\n",
    "                    port_names.add(pn)\n",
    "\n",
    "        return RoutineSignature(\n",
    "            pou_name=pou_name,\n",
    "            reachable_pous=sorted(reachable_names),\n",
    "            called_pou_names=sorted(called_names),\n",
    "            used_variable_names=sorted(used_var_names),\n",
    "            hardware_addresses=sorted(hw_addrs),\n",
    "            port_names=sorted(port_names),\n",
    "        )\n",
    "\n",
    "class RoutineIndex:\n",
    "    def __init__(self, signatures: List[RoutineSignature]):\n",
    "        self.signatures = signatures\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        Path(path).write_text(\n",
    "            json.dumps([s.as_dict() for s in self.signatures], indent=2, ensure_ascii=False),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"RoutineIndex\":\n",
    "        data = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "        sigs = [RoutineSignature(**d) for d in data]\n",
    "        return RoutineIndex(sigs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_from_kg(kg: KGStore, only_pous: Optional[List[str]] = None) -> \"RoutineIndex\":\n",
    "        extractor = SignatureExtractor(kg)\n",
    "        if only_pous is None:\n",
    "            only_pous = sorted(kg._pou_by_name.keys())\n",
    "\n",
    "        sigs: List[RoutineSignature] = []\n",
    "        for name in only_pous:\n",
    "            try:\n",
    "                sigs.append(extractor.extract_signature(name))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return RoutineIndex(sigs)\n",
    "\n",
    "    def find_similar(self, target: RoutineSignature, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        tgt_hw = set(target.hardware_addresses)\n",
    "        tgt_vars = set(target.used_variable_names)\n",
    "        tgt_called = set(target.called_pou_names)\n",
    "\n",
    "        scored: List[Tuple[float, RoutineSignature]] = []\n",
    "        for cand in self.signatures:\n",
    "            cand_hw = set(cand.hardware_addresses)\n",
    "            cand_vars = set(cand.used_variable_names)\n",
    "            cand_called = set(cand.called_pou_names)\n",
    "\n",
    "            sim_hw = jaccard(tgt_hw, cand_hw) if (tgt_hw or cand_hw) else 0.0\n",
    "            sim_vars = jaccard(tgt_vars, cand_vars)\n",
    "            sim_called = jaccard(tgt_called, cand_called)\n",
    "\n",
    "            score = 0.55 * sim_hw + 0.25 * sim_vars + 0.20 * sim_called\n",
    "            scored.append((score, cand))\n",
    "\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [{\"score\": round(s, 4), \"pou_name\": r.pou_name} for s, r in scored[:top_k]]\n",
    "\n",
    "def classify_checkable_sensors(snapshot: SensorSnapshot, sig: RoutineSignature) -> Dict[str, str]:\n",
    "    checkable_set = set(sig.used_variable_names) | set(sig.hardware_addresses)\n",
    "    return {k: (\"checkable\" if k in checkable_set else \"not_checkable\") for k in snapshot.sensor_values.keys()}\n",
    "\n",
    "# Build / Load index\n",
    "from pathlib import Path\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "\n",
    "p = Path(INDEX_PATH)\n",
    "\n",
    "def try_load_index(path: Path):\n",
    "    try:\n",
    "        if not path.exists() or path.stat().st_size == 0:\n",
    "            return None\n",
    "        # BOM-sicher + Whitespace entfernen\n",
    "        raw = path.read_text(encoding=\"utf-8-sig\").strip()\n",
    "        if not raw:\n",
    "            return None\n",
    "        data = json.loads(raw)\n",
    "        sigs = [RoutineSignature(**d) for d in data]\n",
    "        return RoutineIndex(sigs)\n",
    "    except (JSONDecodeError, UnicodeError):\n",
    "        return None\n",
    "\n",
    "routine_index = try_load_index(p)\n",
    "if routine_index is None:\n",
    "    routine_index = RoutineIndex.build_from_kg(kg)\n",
    "    routine_index.save(str(p))\n",
    "    print(\"✅ RoutineIndex neu gebaut & gespeichert:\", p)\n",
    "else:\n",
    "    print(\"✅ RoutineIndex geladen:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f740a5c",
   "metadata": {},
   "source": [
    "## 7) LLM Setup\n",
    "Planner + Text2SPARQL + Answerer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057dbd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API Key erfolgreich aus Datei geladen.\n",
      "✅ LLM Wrapper bereit: openai gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "# === NEU: API Key einlesen ===\n",
    "# Wir lesen den Key aus deiner Datei und setzen ihn als Umgebungsvariable.\n",
    "key_path = r\"C:\\Users\\Alexander Verkhov\\Desktop\\OpenAI API Key.txt\"\n",
    "\n",
    "try:\n",
    "    with open(key_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # .strip() entfernt Leerzeichen/Zeilenumbrüche am Anfang/Ende\n",
    "        api_key = f.read().strip()\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    print(\"✅ OpenAI API Key erfolgreich aus Datei geladen.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fehler beim Laden des API Keys: {e}\")\n",
    "    # Optional: Abbruch, falls Key fehlt\n",
    "    # raise e\n",
    "\n",
    "def get_llm_invoke() -> Callable[[str, str], str]:\n",
    "    if LLM_BACKEND == \"openai\":\n",
    "        try:\n",
    "            from langchain_openai import ChatOpenAI\n",
    "            from langchain_core.messages import SystemMessage, HumanMessage\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                \"Bitte installiere langchain-openai + langchain-core.\\n\"\n",
    "                \"pip install -U langchain-openai langchain-core\"\n",
    "            ) from e\n",
    "\n",
    "        # ChatOpenAI greift nun automatisch auf os.environ[\"OPENAI_API_KEY\"] zu\n",
    "        llm = ChatOpenAI(\n",
    "            model=OPENAI_MODEL, \n",
    "            temperature=OPENAI_TEMPERATURE, \n",
    "            max_tokens=1200\n",
    "        )\n",
    "\n",
    "        def _invoke(system: str, user: str) -> str:\n",
    "            msgs = [SystemMessage(content=system), HumanMessage(content=user)]\n",
    "            return llm.invoke(msgs).content\n",
    "\n",
    "        return _invoke\n",
    "\n",
    "    raise ValueError(\"LLM_BACKEND nicht unterstützt. Setze LLM_BACKEND='openai' oder erweitere den Wrapper.\")\n",
    "\n",
    "llm_invoke = get_llm_invoke()\n",
    "print(\"✅ LLM Wrapper bereit:\", LLM_BACKEND, OPENAI_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad45e0a",
   "metadata": {},
   "source": [
    "## 8) Text2SPARQL (Fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b9906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT2SPARQL_SYSTEM = f\"\"\"\n",
    "Du erzeugst ausschließlich eine SPARQL SELECT Query für einen RDF Knowledge Graph eines SPS Programms.\n",
    "Regeln:\n",
    "- Gib NUR SPARQL zurück (keine Erklärung, kein Markdown).\n",
    "- Nur SELECT (kein INSERT/DELETE/UPDATE, kein SERVICE).\n",
    "- Nutze die Prefixes: rdf, ag, dp, op.\n",
    "Schema Card:\n",
    "{SCHEMA_CARD}\n",
    "\"\"\"\n",
    "\n",
    "def text2sparql(question: str) -> str:\n",
    "    raw = llm_invoke(TEXT2SPARQL_SYSTEM, question)\n",
    "    return extract_sparql_from_llm(raw).strip()\n",
    "\n",
    "def tool_text2sparql_select(question: str, max_rows: int = 50) -> Dict[str, Any]:\n",
    "    q = text2sparql(question)\n",
    "    rows = sparql_select_raw(q, max_rows=max_rows)\n",
    "    return {\"sparql\": q, \"rows\": rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9320f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Baue Vektor-Index auf...\n",
      "⚠️ Keine Dokumente für RAG gefunden.\n",
      "✅ SPARQL Helper + OOP Tool Registry erfolgreich initialisiert.\n",
      "------------------------------\n",
      "Verfügbare Tools:\n",
      "- general_search(name_contains)\n",
      "  Beschreibung: Sucht universell nach POUs, Variablen oder Ports. Gibt Typ und Name zurück.\n",
      "  Wann nutzen: Nutzen, wenn unklar ist, ob ein Name eine POU, eine Variable oder ein Port ist (z.B. bei Punkten im Namen).\n",
      "- list_programs()\n",
      "  Beschreibung: Listet alle verfügbaren Programme im Projekt auf.\n",
      "  Wann nutzen: Wenn der User fragt 'Welche Programme gibt es?' oder einen Einstiegspunkt sucht.\n",
      "- called_pous(program_name)\n",
      "  Beschreibung: Zeigt alle POUs, die von einem Programm aufgerufen werden.\n",
      "  Wann nutzen: Bei Fragen nach Call-Graph, Struktur, 'Wer ruft wen auf?'.\n",
      "- pou_code(pou_name)\n",
      "  Beschreibung: Holt ST-Code, Sprache und Report einer POU.\n",
      "  Wann nutzen: Wenn User nach 'Code', 'Implementierung' oder 'Inhalt' fragt.\n",
      "- search_variables(name_contains)\n",
      "  Beschreibung: Sucht Variablen nach Name (Teilstring).\n",
      "  Wann nutzen: Fragen nach 'Variable', 'Adresse', 'I/O' oder Signalnamen.\n",
      "- variable_trace(name_contains)\n",
      "  Beschreibung: Analysiert Schreib-/Lesezugriffe auf Variablen (Data Flow).\n",
      "  Wann nutzen: Fragen wie 'Woher kommt Signal X?', 'Wer nutzt Variable Y?'.\n",
      "- pou_callers(pou_name)\n",
      "  Beschreibung: Findet heraus, von welchen Programmen oder FBs eine POU aufgerufen wird (Reverse Call Graph).\n",
      "  Wann nutzen: Nutzen bei Fragen wie 'Wer nutzt X?', 'Wo wird X verwendet?' oder allgemein 'Was macht X?' (um den Kontext zu zeigen).\n",
      "- string_triple_search(term, max_hits, context_lines, only_predicates)\n",
      "  Beschreibung: Sucht einen String als Substring in allen Tripeln (Subject, Predicate, Object).\n",
      "  Wann nutzen: Letzter Fallback, wenn strukturierte Tools keine Treffer liefern.\n",
      "\n",
      "✅ Smart ChatBot (mit General Search & Split-Retry) bereit.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# 1. RAG / SEMANTIC SEARCH SETUP\n",
    "# ==========================================\n",
    "\n",
    "def build_vector_index(kg_store, tool_registry):\n",
    "    \"\"\"\n",
    "    Erstellt einen FAISS Index aus POU-Namen und Code-Snippets.\n",
    "    Nutzt die Registry, um den Code zu holen.\n",
    "    \"\"\"\n",
    "    print(\"🔄 Baue Vektor-Index auf...\")\n",
    "    docs = []\n",
    "    \n",
    "    # Wir iterieren über alle bekannten POUs im KG\n",
    "    for pou_name in kg_store._pou_by_name.keys():\n",
    "        try:\n",
    "            # Code über das existierende Tool holen\n",
    "            code_res = tool_registry.execute(\"pou_code\", {\"pou_name\": pou_name})\n",
    "            \n",
    "            # Prüfen ob Ergebnis gültig ist\n",
    "            if isinstance(code_res, list) and code_res and \"code\" in code_res[0]:\n",
    "                code_text = code_res[0][\"code\"]\n",
    "                if code_text:\n",
    "                    # Dokument erstellen: Name + Code\n",
    "                    # Wir kürzen den Code auf 1000 Zeichen für das Embedding\n",
    "                    content = f\"POU Name: {pou_name}\\nCode Content: {code_text[:1000]}\"\n",
    "                    meta = {\"type\": \"POU\", \"name\": pou_name}\n",
    "                    docs.append(Document(page_content=content, metadata=meta))\n",
    "        except Exception:\n",
    "            pass # Fehlerhafte POUs überspringen\n",
    "\n",
    "    if not docs:\n",
    "        print(\"⚠️ Keine Dokumente für RAG gefunden.\")\n",
    "        return None\n",
    "\n",
    "    # Embeddings initialisieren\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    print(f\"✅ Vektor-Index bereit mit {len(docs)} Dokumenten.\")\n",
    "    return vector_store\n",
    "\n",
    "# Index einmalig bauen (benötigt 'kg' aus Cell 21 und 'registry' aus Cell 20)\n",
    "if 'kg' in globals() and 'registry' in globals():\n",
    "    vector_index = build_vector_index(kg, registry)\n",
    "else:\n",
    "    print(\"⚠️ 'kg' oder 'registry' nicht gefunden. Bitte vorherige Zellen ausführen!\")\n",
    "    vector_index = None\n",
    "\n",
    "# Neues Tool für RAG definieren\n",
    "class SemanticSearchTool(BaseAgentTool):\n",
    "    name = \"semantic_search\"\n",
    "    description = \"Sucht semantisch nach POUs oder Logik anhand von Beschreibungen (RAG).\"\n",
    "    usage_guide = \"Wenn der User vage Beschreibungen nutzt (z.B. 'Wie funktioniert der Not-Halt?') und den exakten Namen nicht kennt. Auch gut als Fallback.\"\n",
    "\n",
    "    def __init__(self, vector_store):\n",
    "        self.vs = vector_store\n",
    "\n",
    "    def run(self, query: str, k: int = 3, **kwargs) -> List[Dict[str, Any]]:\n",
    "        if not self.vs:\n",
    "            return [{\"error\": \"Kein Vektor-Index verfügbar.\"}]\n",
    "        \n",
    "        docs = self.vs.similarity_search(query, k=k)\n",
    "        results = []\n",
    "        for d in docs:\n",
    "            results.append({\n",
    "                \"pou_name\": d.metadata.get(\"name\"),\n",
    "                \"snippet\": d.page_content[:300] + \"...\"\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "class GeneralSearchTool(BaseAgentTool):\n",
    "    name = \"general_search\"\n",
    "    description = \"Sucht universell nach POUs, Variablen oder Ports. Gibt Typ und Name zurück.\"\n",
    "    usage_guide = \"Nutzen, wenn unklar ist, ob ein Name eine POU, eine Variable oder ein Port ist (z.B. bei Punkten im Namen).\"\n",
    "\n",
    "    def run(self, name_contains: str, **kwargs) -> List[Dict[str, Any]]:\n",
    "        # Wir suchen nach dem genauen String UND nach der __dot__ Variante für URIs\n",
    "        needle = name_contains.replace('\"', '\\\\\"')\n",
    "        \n",
    "        # Falls der User nach URIs fragt (z.B. Debugging), bauen wir __dot__ ein\n",
    "        needle_dot = needle.replace(\".\", \"__dot__\")\n",
    "        \n",
    "        q = f\"\"\"\n",
    "        SELECT DISTINCT ?name ?type ?category WHERE {{\n",
    "          {{\n",
    "            ?s rdf:type ag:class_POU ;\n",
    "               dp:hasPOUName ?name .\n",
    "            BIND(\"POU\" AS ?category)\n",
    "          }}\n",
    "          UNION\n",
    "          {{\n",
    "            ?s rdf:type ag:class_Variable ;\n",
    "               dp:hasVariableName ?name ;\n",
    "               dp:hasVariableType ?type .\n",
    "            BIND(\"Variable\" AS ?category)\n",
    "          }}\n",
    "          UNION\n",
    "          {{\n",
    "            ?s rdf:type ag:class_Port ;\n",
    "               dp:hasPortName ?name ;\n",
    "               dp:hasPortType ?type .\n",
    "            BIND(\"Port\" AS ?category)\n",
    "          }}\n",
    "          \n",
    "          # Suche sowohl nach normalem Namen als auch URI-Teilen\n",
    "          FILTER(\n",
    "            CONTAINS(LCASE(STR(?name)), LCASE(\"{needle}\")) || \n",
    "            CONTAINS(LCASE(STR(?s)), LCASE(\"{needle_dot}\"))\n",
    "          )\n",
    "        }} LIMIT 20\n",
    "        \"\"\"\n",
    "        return sparql_select_raw(q)\n",
    "\n",
    "\n",
    "class StringTripleSearchTool(BaseAgentTool):\n",
    "    name = \"string_triple_search\"\n",
    "    description = \"Sucht einen String als Substring in allen Tripeln (Subject, Predicate, Object).\"\n",
    "    usage_guide = \"Letzter Fallback, wenn strukturierte Tools keine Treffer liefern.\"\n",
    "\n",
    "    def __init__(self, kg_store=None):\n",
    "        self.kg_store = kg_store if kg_store is not None else globals().get(\"kg\", None)\n",
    "        self.graph = getattr(self.kg_store, \"g\", None) if self.kg_store is not None else globals().get(\"g\", None)\n",
    "\n",
    "    def _short_pred(self, p) -> str:\n",
    "        s = str(p)\n",
    "        if \"#\" in s:\n",
    "            return s.split(\"#\")[-1]\n",
    "        return s.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "    def _resolve_pou_name(self, s) -> Optional[str]:\n",
    "        try:\n",
    "            DP_ns = globals().get(\"DP\", None)\n",
    "            if self.graph is not None and DP_ns is not None:\n",
    "                v = self.graph.value(s, DP_ns.hasPOUName)\n",
    "                if v is not None:\n",
    "                    return str(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        term: str,\n",
    "        max_hits: int = 20,\n",
    "        context_lines: int = 2,\n",
    "        only_predicates: Optional[List[str]] = None,\n",
    "        **kwargs\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        if not self.graph:\n",
    "            return [{\"error\": \"Global graph not found. Run the graph loading cell first.\"}]\n",
    "\n",
    "        import re\n",
    "        from rdflib.term import Literal\n",
    "\n",
    "        pat = re.compile(re.escape(term), re.IGNORECASE)\n",
    "        pred_allow = set(only_predicates) if only_predicates else None\n",
    "\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for s, p, o in self.graph:\n",
    "            p_short = self._short_pred(p)\n",
    "\n",
    "            if pred_allow is not None and p_short not in pred_allow:\n",
    "                continue\n",
    "\n",
    "            in_s = bool(pat.search(str(s)))\n",
    "            in_p = bool(pat.search(str(p)))\n",
    "            in_o = bool(pat.search(str(o)))\n",
    "\n",
    "            if not (in_s or in_p or in_o):\n",
    "                continue\n",
    "\n",
    "            item: Dict[str, Any] = {\n",
    "                \"subject\": str(s),\n",
    "                \"subject_pou_name\": self._resolve_pou_name(s),\n",
    "                \"predicate\": str(p),\n",
    "                \"predicate_short\": p_short,\n",
    "                \"match_in\": [k for k, v in ((\"subject\", in_s), (\"predicate\", in_p), (\"object\", in_o)) if v],\n",
    "            }\n",
    "\n",
    "            if isinstance(o, Literal):\n",
    "                text = str(o)\n",
    "                lines = text.splitlines()\n",
    "                ctxs = []\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    if pat.search(line):\n",
    "                        start = max(0, i - context_lines)\n",
    "                        end = min(len(lines), i + context_lines + 1)\n",
    "                        ctxs.append({\"line\": i + 1, \"context\": \"\\n\".join(lines[start:end])})\n",
    "                        if len(ctxs) >= 5:\n",
    "                            break\n",
    "\n",
    "                item[\"object_type\"] = \"literal\"\n",
    "                item[\"object_preview\"] = text[:400] + (\"...\" if len(text) > 400 else \"\")\n",
    "                if ctxs:\n",
    "                    item[\"contexts\"] = ctxs\n",
    "            else:\n",
    "                item[\"object_type\"] = type(o).__name__\n",
    "                item[\"object_preview\"] = str(o)\n",
    "\n",
    "            results.append(item)\n",
    "            if len(results) >= max_hits:\n",
    "                break\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Registrieren (falls Index existiert und noch nicht registriert)\n",
    "if vector_index:\n",
    "    # KORREKTUR: Wir greifen direkt auf _tools zu, da get_tool nicht existiert\n",
    "    if \"semantic_search\" not in registry._tools:\n",
    "        registry.register(SemanticSearchTool(vector_index))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. CHATBOT KLASSE (History + Dynamic Planner)\n",
    "# ==========================================\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, registry: ToolRegistry, llm_invoke_fn: Callable):\n",
    "        self.registry = registry\n",
    "        self.llm = llm_invoke_fn\n",
    "        self.history = [] \n",
    "\n",
    "    def _get_dynamic_planner_prompt(self, retry_hint: str = \"\") -> str:\n",
    "        tool_docs = self.registry.get_system_prompt_part()\n",
    "        \n",
    "        heuristics = []\n",
    "        for tool in self.registry._tools.values():\n",
    "            if tool.usage_guide:\n",
    "                heuristics.append(f\"- {tool.usage_guide} -> {tool.name}\")\n",
    "        \n",
    "        retry_msg = f\"\\nACHTUNG - VORHERIGER VERSUCH GESCHEITERT:\\n{retry_hint}\\n\" if retry_hint else \"\"\n",
    "\n",
    "        return f\"\"\"\n",
    "Du bist ein Planner für einen PLC Knowledge-Graph ChatBot.\n",
    "Zerlege die Anfrage in Tool-Aufrufe.\n",
    "\n",
    "{tool_docs}\n",
    "\n",
    "STRATEGIE BEI PUNKTEN (z.B. \"GVL.Start\"):\n",
    "- Ein Punkt deutet oft auf Variable, Port oder Instanz hin.\n",
    "- Nutze 'general_search', um herauszufinden, was es ist (POU vs. Variable).\n",
    "- Wenn du sicher bist, dass es eine Variable ist -> 'variable_trace' oder 'search_variables'.\n",
    "\n",
    "Heuristiken:\n",
    "{chr(10).join(heuristics)}\n",
    "- Wenn nach mehreren Tool Aufrufen keine Treffer kommen, nutze string_triple_search(term) als letzten Fallback\n",
    "- Sonst -> text2sparql_select\n",
    "\n",
    "{retry_msg}\n",
    "\n",
    "Ausgabeformat (NUR JSON):\n",
    "{{\n",
    "  \"steps\": [\n",
    "    {{\"tool\": \"tool_name\", \"args\": {{\"arg1\": \"wert1\"}} }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    def _is_result_empty(self, results: Dict[str, Any]) -> bool:\n",
    "        if not results: return True\n",
    "        for val in results.values():\n",
    "            if isinstance(val, dict) and \"error\" in val: return False\n",
    "            if isinstance(val, list) and len(val) > 0: return False\n",
    "            if val: return False\n",
    "        return True\n",
    "\n",
    "    def _generate_split_hint(self, user_msg: str) -> str:\n",
    "        \"\"\"\n",
    "        Analysiert die User-Anfrage nach Punkten und generiert Suchvorschläge für die Teile.\n",
    "        \"\"\"\n",
    "        # Suche nach Wörtern mit Punkt (z.B. GVL.Start)\n",
    "        import re\n",
    "        candidates = re.findall(r\"([a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+)\", user_msg)\n",
    "        \n",
    "        hint = \"Die vorherige Suche lieferte KEINE Ergebnisse.\\n\"\n",
    "        \n",
    "        if candidates:\n",
    "            for c in candidates:\n",
    "                parts = c.split('.')\n",
    "                hint += f\"Der Begriff '{c}' enthält einen Punkt. Falls die exakte Suche fehlschlug, suche nach den Teilen einzeln:\\n\"\n",
    "                hint += f\" -> Versuche general_search('{parts[0]}') (Container/Instanz?)\\n\"\n",
    "                hint += f\" -> Versuche general_search('{parts[1]}') (Element/Port?)\\n\"\n",
    "        else:\n",
    "            hint += \"Versuche den Suchbegriff zu verkürzen oder 'general_search' zu nutzen.\"\n",
    "            \n",
    "        return hint\n",
    "\n",
    "\n",
    "    def _extract_identifier_candidates(self, user_msg: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extrahiert mögliche Identifier aus der Frage.\n",
    "        Wird nur für den letzten String Fallback genutzt.\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        stop = {\n",
    "            \"wo\",\"wird\",\"ist\",\"sind\",\"warum\",\"wie\",\"was\",\"wer\",\"welche\",\"welcher\",\"welches\",\n",
    "            \"implementiert\",\"implementierung\",\"genau\",\"bitte\",\"frage\",\"antwort\",\n",
    "            \"in\",\"im\",\"am\",\"an\",\"auf\",\"von\",\"zu\",\"mit\",\"ohne\",\"für\",\"und\",\"oder\",\"der\",\"die\",\"das\",\"ein\",\"eine\",\"einer\",\"eines\",\n",
    "        }\n",
    "\n",
    "        candidates: List[str] = []\n",
    "\n",
    "        # 1) Inhalte in Quotes priorisieren\n",
    "        candidates += re.findall(r\"'([^']{2,80})'\", user_msg)\n",
    "        candidates += re.findall(r'\"([^\"]{2,80})\"', user_msg)\n",
    "\n",
    "        # 2) Tokens mit Zahl (sehr typisch für Skills, z.B. TestSkill3)\n",
    "        candidates += re.findall(r\"\\b[A-Za-z_]*[A-Za-z]+[A-Za-z_]*\\d+[A-Za-z0-9_]*\\b\", user_msg)\n",
    "\n",
    "        # 3) Allgemeine Identifier (Fallback)\n",
    "        candidates += re.findall(r\"\\b[A-Za-z_][A-Za-z0-9_]{2,}\\b\", user_msg)\n",
    "\n",
    "        out: List[str] = []\n",
    "        seen = set()\n",
    "        for t in candidates:\n",
    "            t = t.strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            if t.lower() in stop:\n",
    "                continue\n",
    "            key = t.lower()\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            out.append(t)\n",
    "\n",
    "        return out[:5]\n",
    "\n",
    "\n",
    "    def chat(self, user_msg: str, debug: bool = True):\n",
    "        # 1. Context & Plan\n",
    "        history_text = \"Vergangener Chat:\\n\" + \"\\n\".join([f\"{r}: {m}\" for r, m in self.history[-3:]])\n",
    "        full_prompt_input = f\"{history_text}\\nAktuelle Anfrage: {user_msg}\"\n",
    "        \n",
    "        # Helper JSON Parser\n",
    "        def safe_json_loads(s):\n",
    "            import re\n",
    "            t = s.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            m = re.search(r\"(\\{.*\\})\", t, flags=re.DOTALL)\n",
    "            return json.loads(m.group(1) if m else t)\n",
    "\n",
    "        # === 1. VERSUCH ===\n",
    "        planner_sys = self._get_dynamic_planner_prompt()\n",
    "        plan_raw = self.llm(planner_sys, full_prompt_input)\n",
    "        \n",
    "        try:\n",
    "            plan = safe_json_loads(plan_raw)\n",
    "        except:\n",
    "            plan = {\"steps\": [{\"tool\": \"text2sparql_select\", \"args\": {\"question\": user_msg}}]}\n",
    "\n",
    "        results = {}\n",
    "        for i, step in enumerate(plan.get(\"steps\", []), 1):\n",
    "            t_name = step.get(\"tool\")\n",
    "            t_args = step.get(\"args\", {})\n",
    "            results[f\"step_{i}_{t_name}\"] = self.registry.execute(t_name, t_args)\n",
    "\n",
    "        # === 2. VERSUCH (Retry mit Split-Logik) ===\n",
    "        if self._is_result_empty(results):\n",
    "            if debug: print(\"⚠️ Keine Ergebnisse. Starte Smart-Retry (Split Search)...\")\n",
    "            \n",
    "            # Hier generieren wir den schlauen Hinweis für den Planner\n",
    "            split_hint = self._generate_split_hint(user_msg)\n",
    "            \n",
    "            planner_sys_retry = self._get_dynamic_planner_prompt(retry_hint=split_hint)\n",
    "            plan_retry_raw = self.llm(planner_sys_retry, full_prompt_input)\n",
    "            \n",
    "            try:\n",
    "                new_plan = safe_json_loads(plan_retry_raw)\n",
    "                if new_plan.get(\"steps\"):\n",
    "                    plan = new_plan\n",
    "                    results = {}\n",
    "                    for i, step in enumerate(plan.get(\"steps\", []), 1):\n",
    "                        t_name = step.get(\"tool\")\n",
    "                        t_args = step.get(\"args\", {})\n",
    "                        results[f\"step_{i}_{t_name}_retry\"] = self.registry.execute(t_name, t_args)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        # === 3. VERSUCH (Deterministischer String Fallback) ===\n",
    "        if self._is_result_empty(results):\n",
    "            if debug:\n",
    "                print(\"⚠️ Noch immer keine Ergebnisse. Starte String-Fallback (Triple Scan)...\")\n",
    "\n",
    "            terms = self._extract_identifier_candidates(user_msg)\n",
    "            if not terms:\n",
    "                terms = [user_msg.strip()[:50]]\n",
    "\n",
    "            for term in terms:\n",
    "                res3 = self.registry.execute(\n",
    "                    \"string_triple_search\",\n",
    "                    {\n",
    "                        \"term\": term,\n",
    "                        \"max_hits\": 20,\n",
    "                        \"context_lines\": 3,\n",
    "                        \"only_predicates\": [\"dp_hasPOUCode\", \"dp_hasProgramCode\", \"dp_hasExpressionText\"],\n",
    "                    },\n",
    "                )\n",
    "                results[f\"step_string_triple_search_{term}\"] = res3\n",
    "\n",
    "                if isinstance(res3, list) and res3 and not (isinstance(res3[0], dict) and \"error\" in res3[0]):\n",
    "                    break\n",
    "\n",
    "\n",
    "        # 4. Antwort\n",
    "        answer_sys = \"\"\"\n",
    "        Du bist ein hilfreicher SPS-Experte. Nutze NUR die Tool-Ergebnisse.\n",
    "        Erkläre Zusammenhänge.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"history\": self.history[-3:], \n",
    "            \"current_question\": user_msg,\n",
    "            \"final_plan\": plan,\n",
    "            \"tool_results\": results\n",
    "        }\n",
    "        answer = self.llm(answer_sys, json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "\n",
    "        self.history.append((\"User\", user_msg))\n",
    "        self.history.append((\"AI\", answer))\n",
    "\n",
    "        return {\"answer\": answer, \"plan\": plan, \"results\": results}\n",
    "\n",
    "# Standard Tools\n",
    "registry.register(GeneralSearchTool())\n",
    "registry.register(ListProgramsTool())\n",
    "registry.register(CalledPousTool())\n",
    "registry.register(PouCodeTool())\n",
    "registry.register(SearchVariablesTool())\n",
    "registry.register(VariableTraceTool())\n",
    "registry.register(PouCallersTool())\n",
    "registry.register(StringTripleSearchTool(kg if \"kg\" in globals() else None))\n",
    "\n",
    "print(\"✅ SPARQL Helper + OOP Tool Registry erfolgreich initialisiert.\")\n",
    "print(\"-\" * 30)\n",
    "print(registry.get_system_prompt_part())\n",
    "\n",
    "# Bot instanziieren\n",
    "bot = ChatBot(registry, llm_invoke)\n",
    "print(\"✅ Smart ChatBot (mit General Search & Split-Retry) bereit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90278312",
   "metadata": {},
   "source": [
    "## 11) Chat UI (ipywidgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e81b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df91d914ff7349a9a51e945fdef5b751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Debug (Plan + Tool Results anzeigen)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd67a783ac34a569902227a497f2334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(height='90px', width='100%'), placeholder=\"Frage stellen... (z.B. 'Was macht …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c31bfd03e6346f195c7c11eb465a1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803e85893ff14888a37ed357f31410d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "# 1. Widgets definieren\n",
    "debug_toggle = widgets.Checkbox(value=True, description=\"Debug (Plan + Tool Results anzeigen)\")\n",
    "input_box = widgets.Textarea(\n",
    "    placeholder=\"Frage stellen... (z.B. 'Was macht der Not-Halt?')\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"90px\")\n",
    ")\n",
    "send_btn = widgets.Button(description=\"Send\", button_style=\"primary\")\n",
    "out = widgets.Output()\n",
    "\n",
    "display(debug_toggle, input_box, send_btn, out)\n",
    "\n",
    "# 2. Event-Handler Logik\n",
    "def on_send(b):\n",
    "    # Sofort sperren, um Doppelklicks zu verhindern\n",
    "    b.disabled = True\n",
    "    \n",
    "    try:\n",
    "        user_msg = input_box.value.strip()\n",
    "        out.clear_output(wait=True) # wait=True verhindert Flackern\n",
    "        \n",
    "        if not user_msg:\n",
    "            return\n",
    "\n",
    "        with out:\n",
    "            print(f\"User: {user_msg}\")\n",
    "            \n",
    "            # Bot aufrufen\n",
    "            try:\n",
    "                # Hier greifen wir auf dein globales 'bot' Objekt zu\n",
    "                resp = bot.chat(user_msg, debug=debug_toggle.value)\n",
    "\n",
    "                display(Markdown(\"### Antwort\"))\n",
    "                print(resp[\"answer\"])\n",
    "\n",
    "                if debug_toggle.value:\n",
    "                    display(Markdown(\"### Plan\"))\n",
    "                    print(json.dumps(resp[\"plan\"], ensure_ascii=False, indent=2))\n",
    "\n",
    "                    display(Markdown(\"### Tool Results (gekürzt)\"))\n",
    "                    print(json.dumps(resp[\"results\"], ensure_ascii=False, indent=2)[:8000])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Ein Fehler ist aufgetreten: {e}\")\n",
    "                \n",
    "    finally:\n",
    "        # Button am Ende immer wieder freigeben\n",
    "        b.disabled = False\n",
    "\n",
    "# 3. Handler registrieren\n",
    "# Wichtig: Wir definieren den Button oben neu, also ist er \"frisch\". \n",
    "# Ein einfaches .on_click reicht.\n",
    "send_btn.on_click(on_send)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055455b",
   "metadata": {},
   "source": [
    "## 12) Quick Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b9309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      " Welche Programme gibt es?\n",
      "Fehler: name 'chat_once' is not defined\n",
      "\n",
      "---\n",
      " Welche POUs ruft HRL_SkillSet auf?\n",
      "Fehler: name 'chat_once' is not defined\n",
      "\n",
      "---\n",
      " Zeig mir den Code von JobMethode_Schablone\n",
      "Fehler: name 'chat_once' is not defined\n",
      "\n",
      "---\n",
      " Suche Variablen, die 'NotAus' enthalten\n",
      "Fehler: name 'chat_once' is not defined\n",
      "\n",
      "---\n",
      " Trace für DI04_EncoderStart02\n",
      "Fehler: name 'chat_once' is not defined\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"Welche Programme gibt es?\",\n",
    "    \"Welche POUs ruft HRL_SkillSet auf?\",\n",
    "    \"Zeig mir den Code von JobMethode_Schablone\",\n",
    "    \"Suche Variablen, die 'NotAus' enthalten\",\n",
    "    \"Trace für DI04_EncoderStart02\",\n",
    "]\n",
    "\n",
    "for q in tests:\n",
    "    print(\"\\n---\\n\", q)\n",
    "    try:\n",
    "        resp = chat_once(q, debug=False)\n",
    "        print(resp[\"answer\"][:700])\n",
    "    except Exception as e:\n",
    "        print(\"Fehler:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
